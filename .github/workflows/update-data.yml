name: Daily Investment Calendar Update

on:
  schedule:
    # æ¯å¤©åŒ—äº¬æ—¶é—´ 6:00 (UTC 22:00) æ‰§è¡Œ
    - cron: '0 22 * * *'
  workflow_dispatch: # å…è®¸æ‰‹åŠ¨è§¦å‘

permissions:
  contents: write

jobs:
  update-data:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        pip install -r scripts/requirements.txt
    
    - name: Ensure directory structure
      run: |
        mkdir -p data/web
        mkdir -p data/active/current
        mkdir -p data/active/previous
        echo "ğŸ“ å½“å‰ç›®å½•ç»“æ„ï¼š"
        ls -la
        echo "ğŸ“ Dataç›®å½•ç»“æ„ï¼š"
        ls -la data/ || echo "Dataç›®å½•ä¸å­˜åœ¨"
        echo "ğŸ“ Archivedç›®å½•ç»“æ„ï¼š"
        ls -la data/archived/ || echo "Archivedç›®å½•ä¸å­˜åœ¨"
    
    - name: Check current data status
      id: check_data
      run: |
        echo "ğŸ” æ£€æŸ¥å½“å‰æ•°æ®çŠ¶æ€..."
        
        current_count=0
        for platform in cls jiuyangongshe tonghuashun investing eastmoney; do
          file_path="data/active/current/${platform}.txt"
          if [ -f "$file_path" ] && [ -s "$file_path" ]; then
            current_count=$((current_count + 1))
            echo "âœ… å‘ç° ${platform}.txt ($(stat -c%s "$file_path") bytes)"
          else
            echo "âŒ ç¼ºå°‘æˆ–ä¸ºç©º: ${platform}.txt"
          fi
        done
        
        echo "ğŸ“Š å½“å‰æ•°æ®æ–‡ä»¶: $current_count/5"
        
        if [ $current_count -ge 3 ]; then
          echo "run_mode=daily" >> $GITHUB_OUTPUT
          echo "ğŸ”„ ä½¿ç”¨æ—¥å¸¸æ›´æ–°æ¨¡å¼"
        else
          echo "run_mode=first-run" >> $GITHUB_OUTPUT
          echo "ğŸš€ ä½¿ç”¨é¦–æ¬¡è¿è¡Œæ¨¡å¼"
        fi
    
    - name: Run investment calendar
      run: |
        mode="${{ steps.check_data.outputs.run_mode }}"
        echo "ğŸš€ æ‰§è¡Œæ¨¡å¼: $mode"
        
        if [ "$mode" == "daily" ]; then
          python scripts/daily_calendar.py --daily
        else
          if [ ! -f "data/archived/historical_summary.txt" ]; then
            echo "ğŸ“š å¼€å§‹é‡‡é›†å†å²æ•°æ®..."
            python scripts/historical_collector.py
          fi
          echo "ğŸš€ å¼€å§‹é¦–æ¬¡è¿è¡Œ..."
          python scripts/daily_calendar.py --first-run
        fi
        
        # æ£€æŸ¥æ•°æ®é‡‡é›†ç»“æœ
        echo "ğŸ“Š æ•°æ®é‡‡é›†åçš„currentç›®å½•:"
        ls -la data/active/current/ || echo "Currentç›®å½•ä¸å­˜åœ¨"
        
        echo "ğŸ“„ Currentç›®å½•æ–‡ä»¶å¤§å°:"
        du -h data/active/current/*.txt 2>/dev/null || echo "æ— txtæ–‡ä»¶"
    
    - name: Create comprehensive data converter script
      run: |
        cat > convert_data.py << 'EOF'
        import json
        import os
        import glob
        from datetime import datetime

        def convert_platform_data(platform):
            source_file = f'data/active/current/{platform}.txt'
            target_file = f'data/web/{platform}.json'
            
            if not os.path.exists(source_file):
                print(f'âš ï¸ {platform} æºæ–‡ä»¶ä¸å­˜åœ¨')
                return False
            
            try:
                with open(source_file, 'r', encoding='utf-8') as f:
                    data = json.loads(f.read())
                
                platform_names = {
                    'cls': 'è´¢è”ç¤¾',
                    'jiuyangongshe': 'éŸ­ç ”å…¬ç¤¾', 
                    'tonghuashun': 'åŒèŠ±é¡º',
                    'investing': 'è‹±ä¸ºè´¢æƒ…',
                    'eastmoney': 'ä¸œæ–¹è´¢å¯Œ'
                }
                
                web_data = {
                    'platform': platform,
                    'platform_name': platform_names.get(platform, platform),
                    'data_type': 'ACTIVE',
                    'total_events': data.get('total_events', 0),
                    'last_updated': data.get('last_updated', datetime.now().isoformat()),
                    'events': []
                }
                
                for event_data in data.get('events', []):
                    web_event = {
                        'id': event_data.get('event_id', ''),
                        'platform': event_data.get('platform', ''),
                        'title': event_data.get('title', ''),
                        'event_date': event_data.get('event_date', ''),
                        'event_time': event_data.get('event_time'),
                        'event_datetime': event_data.get('event_datetime'),
                        'content': event_data.get('content'),
                        'category': event_data.get('category'),
                        'importance': event_data.get('importance', 1),
                        'country': event_data.get('country'),
                        'city': event_data.get('city'),
                        'is_new': event_data.get('is_new', False),
                        'discovery_date': event_data.get('discovery_date'),
                        'data_status': event_data.get('data_status', 'ACTIVE'),
                        'stocks': event_data.get('stocks', []),
                        'themes': event_data.get('themes', []),
                        'concepts': event_data.get('concepts', [])
                    }
                    web_data['events'].append({k: v for k, v in web_event.items() if v is not None})
                
                with open(target_file, 'w', encoding='utf-8') as f:
                    json.dump(web_data, f, ensure_ascii=False, indent=2)
                
                print(f'âœ… {platform} è½¬æ¢å®Œæˆ: {len(web_data["events"])} ä¸ªäº‹ä»¶')
                return True
                
            except Exception as e:
                print(f'âŒ {platform} è½¬æ¢å¤±è´¥: {e}')
                return False

        def convert_archived_data():
            print("ğŸ“š å¼€å§‹è½¬æ¢å†å²å½’æ¡£æ•°æ®...")
            
            archived_path = "data/archived"
            if not os.path.exists(archived_path):
                print("âš ï¸ å½’æ¡£ç›®å½•ä¸å­˜åœ¨")
                return 0
            
            platform_names = {
                'cls': 'è´¢è”ç¤¾',
                'jiuyangongshe': 'éŸ­ç ”å…¬ç¤¾', 
                'tonghuashun': 'åŒèŠ±é¡º',
                'investing': 'è‹±ä¸ºè´¢æƒ…',
                'eastmoney': 'ä¸œæ–¹è´¢å¯Œ'
            }
            
            platforms = ['cls', 'jiuyangongshe', 'tonghuashun', 'investing', 'eastmoney']
            converted_files = 0
            
            for year_dir in os.listdir(archived_path):
                if not year_dir.isdigit():
                    continue
                    
                year_path = os.path.join(archived_path, year_dir)
                if not os.path.isdir(year_path):
                    continue
                
                print(f"ğŸ“… å¤„ç† {year_dir} å¹´æ•°æ®...")
                
                for month_dir in os.listdir(year_path):
                    if not month_dir.endswith('æœˆ'):
                        continue
                        
                    month_path = os.path.join(year_path, month_dir)
                    if not os.path.isdir(month_path):
                        continue
                    
                    month_num = month_dir.replace('æœˆ', '').zfill(2)
                    print(f"  ğŸ“Š å¤„ç† {year_dir}-{month_num} æ•°æ®...")
                    
                    for platform in platforms:
                        source_file = os.path.join(month_path, f"{platform}.txt")
                        if not os.path.exists(source_file):
                            continue
                        
                        try:
                            with open(source_file, 'r', encoding='utf-8') as f:
                                data = json.loads(f.read())
                            
                            target_file = f"data/web/{platform}_history_{year_dir}_{month_num}.json"
                            
                            web_data = {
                                'platform': platform,
                                'platform_name': platform_names.get(platform, platform),
                                'data_type': 'HISTORICAL',
                                'year': int(year_dir),
                                'month': int(month_num),
                                'total_events': data.get('total_events', 0),
                                'events': []
                            }
                            
                            for event_data in data.get('events', []):
                                web_event = {
                                    'id': event_data.get('event_id', ''),
                                    'platform': event_data.get('platform', ''),
                                    'title': event_data.get('title', ''),
                                    'event_date': event_data.get('event_date', ''),
                                    'event_time': event_data.get('event_time'),
                                    'content': event_data.get('content'),
                                    'category': event_data.get('category'),
                                    'importance': event_data.get('importance', 1),
                                    'country': event_data.get('country'),
                                    'city': event_data.get('city'),
                                    'data_status': 'ARCHIVED',
                                    'stocks': event_data.get('stocks', []),
                                    'themes': event_data.get('themes', []),
                                    'concepts': event_data.get('concepts', [])
                                }
                                web_data['events'].append({k: v for k, v in web_event.items() if v is not None})
                            
                            with open(target_file, 'w', encoding='utf-8') as f:
                                json.dump(web_data, f, ensure_ascii=False, indent=2)
                            
                            converted_files += 1
                            print(f"    âœ… {platform} {year_dir}-{month_num}: {len(web_data['events'])} ä¸ªäº‹ä»¶")
                            
                        except Exception as e:
                            print(f"    âŒ {platform} {year_dir}-{month_num} è½¬æ¢å¤±è´¥: {e}")
            
            print(f"ğŸ“š å†å²æ•°æ®è½¬æ¢å®Œæˆï¼Œå…±è½¬æ¢ {converted_files} ä¸ªæ–‡ä»¶")
            return converted_files

        def generate_historical_index():
            print("ğŸ“‹ ç”Ÿæˆå†å²æ•°æ®ç´¢å¼•...")
            
            historical_files = glob.glob("data/web/*_history_*.json")
            
            index = {
                'generated_at': datetime.now().isoformat(),
                'total_files': len(historical_files),
                'available_periods': [],
                'platforms': {}
            }
            
            periods = set()
            platform_data = {}
            
            for file_path in historical_files:
                try:
                    filename = os.path.basename(file_path)
                    parts = filename.replace('.json', '').split('_')
                    if len(parts) >= 4:
                        platform = parts[0]
                        year = parts[2]
                        month = parts[3]
                        period = f"{year}-{month}"
                        
                        periods.add(period)
                        
                        if platform not in platform_data:
                            platform_data[platform] = []
                        platform_data[platform].append(period)
                        
                except Exception as e:
                    print(f"è§£ææ–‡ä»¶åå¤±è´¥: {filename}, {e}")
            
            index['available_periods'] = sorted(list(periods))
            index['platforms'] = {k: sorted(v) for k, v in platform_data.items()}
            
            with open('data/web/historical_index.json', 'w', encoding='utf-8') as f:
                json.dump(index, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“‹ å†å²ç´¢å¼•ç”Ÿæˆå®Œæˆ: {len(periods)} ä¸ªæ—¶æœŸï¼Œ{len(platform_data)} ä¸ªå¹³å°")

        def convert_change_reports():
            print("ğŸ“Š è½¬æ¢å˜æ›´æŠ¥å‘Š...")
            
            platform_names = {
                'cls': 'è´¢è”ç¤¾',
                'jiuyangongshe': 'éŸ­ç ”å…¬ç¤¾', 
                'tonghuashun': 'åŒèŠ±é¡º',
                'investing': 'è‹±ä¸ºè´¢æƒ…',
                'eastmoney': 'ä¸œæ–¹è´¢å¯Œ'
            }
            
            change_files = glob.glob('data/active/current/change_report_*.txt')
            for change_file in change_files:
                try:
                    with open(change_file, 'r', encoding='utf-8') as f:
                        change_data = json.loads(f.read())
                    
                    web_change_data = {
                        'detection_time': change_data.get('detection_time'),
                        'summary': change_data.get('summary', {}),
                        'platforms': {},
                        'top_new_events': []
                    }
                    
                    for platform, changes in change_data.get('platforms', {}).items():
                        web_change_data['platforms'][platform] = {
                            'name': platform_names.get(platform, platform),
                            'new_events': changes.get('new_events', 0),
                            'updated_events': changes.get('updated_events', 0),
                            'cancelled_events': changes.get('cancelled_events', 0),
                            'sample_new_titles': changes.get('sample_new_titles', [])
                        }
                    
                    for event in change_data.get('top_new_events', []):
                        web_event = {
                            'platform': event.get('platform'),
                            'platform_name': platform_names.get(event.get('platform', ''), event.get('platform', '')),
                            'date': event.get('date'),
                            'title': event.get('title'),
                            'importance': event.get('importance'),
                            'country': event.get('country')
                        }
                        web_change_data['top_new_events'].append(web_event)
                    
                    change_filename = os.path.basename(change_file).replace('.txt', '.json')
                    with open(f'data/web/{change_filename}', 'w', encoding='utf-8') as f:
                        json.dump(web_change_data, f, ensure_ascii=False, indent=2)
                    
                    print(f'âœ… å˜æ›´æŠ¥å‘Šè½¬æ¢å®Œæˆ: {change_filename}')
                    
                except Exception as e:
                    print(f'âŒ å˜æ›´æŠ¥å‘Šè½¬æ¢å¤±è´¥: {e}')

        def main():
            print("ğŸ”„ å¼€å§‹æ•°æ®è½¬æ¢...")
            
            platforms = ['cls', 'jiuyangongshe', 'tonghuashun', 'investing', 'eastmoney']
            current_converted = 0
            
            print("ğŸ“Š è½¬æ¢å½“å‰æ•°æ®...")
            for platform in platforms:
                if convert_platform_data(platform):
                    current_converted += 1
            
            historical_converted = convert_archived_data()
            generate_historical_index()
            convert_change_reports()
            
            print("ğŸ“‹ ç”Ÿæˆå…ƒæ•°æ®...")
            metadata = {
                'last_updated': datetime.now().isoformat(),
                'total_events': 0,
                'platforms': {},
                'has_historical_data': historical_converted > 0,
                'historical_files': historical_converted
            }
            
            platform_names = {
                'cls': 'è´¢è”ç¤¾',
                'jiuyangongshe': 'éŸ­ç ”å…¬ç¤¾', 
                'tonghuashun': 'åŒèŠ±é¡º',
                'investing': 'è‹±ä¸ºè´¢æƒ…',
                'eastmoney': 'ä¸œæ–¹è´¢å¯Œ'
            }
            
            for platform in platforms:
                json_file = f'data/web/{platform}.json'
                if os.path.exists(json_file):
                    try:
                        with open(json_file, 'r', encoding='utf-8') as f:
                            data = json.loads(f.read())
                            event_count = data.get('total_events', 0)
                            metadata['total_events'] += event_count
                            metadata['platforms'][platform] = {
                                'name': data.get('platform_name', platform),
                                'event_count': event_count
                            }
                    except Exception as e:
                        print(f'è¯»å–{platform}å…ƒæ•°æ®å¤±è´¥: {e}')
            
            with open('data/web/metadata.json', 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            print(f'ğŸ‰ è½¬æ¢å®Œæˆ!')
            print(f'å½“å‰æ•°æ®: {current_converted} ä¸ªå¹³å°')
            print(f'å†å²æ•°æ®: {historical_converted} ä¸ªæ–‡ä»¶')
            print(f'æ€»äº‹ä»¶æ•°: {metadata["total_events"]}')

        if __name__ == '__main__':
            main()
        EOF
    
    - name: Convert data for web
      run: |
        echo "ğŸ”„ è½¬æ¢æ•°æ®ä¸ºWebæ ¼å¼..."
        mkdir -p data/web
        python3 convert_data.py
        
        echo "ğŸ“Š Webæ•°æ®ç”Ÿæˆç»“æœ:"
        ls -la data/web/ || echo "Webç›®å½•ä¸å­˜åœ¨"
        
        echo "ğŸ“„ ç”Ÿæˆçš„JSONæ–‡ä»¶ç»Ÿè®¡:"
        current_files=$(ls data/web/*.json 2>/dev/null | grep -v history | grep -v index | grep -v metadata | grep -v change_report | wc -l)
        history_files=$(ls data/web/*_history_*.json 2>/dev/null | wc -l)
        other_files=$(ls data/web/*index*.json data/web/metadata.json data/web/change_report*.json 2>/dev/null | wc -l)
        
        echo "  - å½“å‰æ•°æ®: $current_files ä¸ª"
        echo "  - å†å²æ•°æ®: $history_files ä¸ª"
        echo "  - å…¶ä»–æ–‡ä»¶: $other_files ä¸ª"
        
        echo "ğŸ“‹ å†å²æ•°æ®æ–‡ä»¶ç¤ºä¾‹:"
        ls data/web/*_history_*.json 2>/dev/null | head -5 || echo "æ— å†å²æ•°æ®æ–‡ä»¶"
    
    - name: Configure Git
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
    
    - name: Commit and push changes
      run: |
        echo "ğŸ“ å‡†å¤‡æäº¤æ›´æ”¹..."
        
        echo "ğŸ“Š Currentç›®å½•å†…å®¹:"
        ls -la data/active/current/ || echo "Currentç›®å½•ä¸å­˜åœ¨"
        
        git add data/active/current/*.txt 2>/dev/null || echo "æ— currentæ•°æ®æ–‡ä»¶"
        git add data/web/ || echo "æ— webæ•°æ®"
        git add data/active/current/metadata.txt 2>/dev/null || echo "æ— å…ƒæ•°æ®æ–‡ä»¶"
        
        if [ -n "$(git status --porcelain)" ]; then
          echo "ğŸ“Š å‘ç°ä»¥ä¸‹å˜æ›´:"
          git status --porcelain | head -20
          
          total_changes=$(git status --porcelain | wc -l)
          if [ $total_changes -gt 20 ]; then
            echo "... è¿˜æœ‰ $((total_changes - 20)) ä¸ªå˜æ›´æ–‡ä»¶"
          fi
          
          git commit -m "ğŸ“Š Daily data update - $(date +'%Y-%m-%d %H:%M:%S')"
          git push
          echo "âœ… å˜æ›´å·²æäº¤å¹¶æ¨é€"
        else
          echo "â„¹ï¸ æ²¡æœ‰éœ€è¦æäº¤çš„å˜æ›´"
        fi
    
    - name: Deployment summary
      run: |
        echo "ğŸ‰ æ•°æ®æ›´æ–°å®Œæˆ!"
        echo "ğŸ“Š æœ€ç»ˆçŠ¶æ€:"
        echo "  - è¿è¡Œæ¨¡å¼: ${{ steps.check_data.outputs.run_mode }}"
        
        current_count=$(ls data/active/current/*.txt 2>/dev/null | wc -l)
        web_count=$(ls data/web/*.json 2>/dev/null | wc -l)
        history_count=$(ls data/web/*_history_*.json 2>/dev/null | wc -l)
        
        echo "  - Currentæ•°æ®: $current_count ä¸ªæ–‡ä»¶"
        echo "  - Webæ•°æ®: $web_count ä¸ªJSONæ–‡ä»¶"
        echo "  - å†å²æ•°æ®: $history_count ä¸ªå†å²æ–‡ä»¶"
        echo "  - ç½‘ç«™åœ°å€: https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/"
        
        if [ -f "data/web/metadata.json" ]; then
          echo "ğŸ“ˆ æ•°æ®ç»Ÿè®¡:"
          python3 -c "
import json
try:
    with open('data/web/metadata.json', 'r') as f:
        metadata = json.load(f)
    print('  - æ€»äº‹ä»¶æ•°: ' + str(metadata.get('total_events', 0)))
    print('  - æ´»è·ƒå¹³å°: ' + str(len(metadata.get('platforms', {}))))
    print('  - å†å²æ–‡ä»¶: ' + str(metadata.get('historical_files', 0)))
    print('  - æœ€åæ›´æ–°: ' + metadata.get('last_updated', 'æœªçŸ¥'))
except Exception as e:
    print('  - æ— æ³•è¯»å–å…ƒæ•°æ®: ' + str(e))
"
        fi
        
        if [ -f "data/web/historical_index.json" ]; then
          echo "ğŸ“… å†å²æ•°æ®ç´¢å¼•:"
          python3 -c "
import json
try:
    with open('data/web/historical_index.json', 'r') as f:
        index = json.load(f)
    periods = index.get('available_periods', [])
    print('  - å¯ç”¨æ—¶æœŸ: ' + str(len(periods)) + ' ä¸ª')
    if periods:
        print('  - æ—¶é—´èŒƒå›´: ' + periods[0] + ' è‡³ ' + periods[-1])
        recent = periods[-3:] if len(periods) >= 3 else periods
        print('  - æœ€æ–°æ—¶æœŸ: ' + str(recent))
except Exception as e:
    print('  - æ— æ³•è¯»å–å†å²ç´¢å¼•: ' + str(e))
"
        fi
        
        echo ""
        echo "ğŸŒ è®¿é—®ç½‘ç«™æŸ¥çœ‹æ•°æ®æ›´æ–°ç»“æœï¼"
