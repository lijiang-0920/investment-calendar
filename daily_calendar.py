#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŠ•èµ„æ—¥å†æ—¥å¸¸æ•°æ®é‡‡é›†ä¸å˜æ›´æ£€æµ‹
æ¯æ—¥è¿è¡Œï¼Œé‡‡é›†æœªæ¥æ•°æ®å¹¶æ£€æµ‹å˜æ›´
"""

import requests
import json
import hashlib
import time
import os
import re
import shutil
import calendar
import urllib.parse
from datetime import datetime, timedelta
from typing import List, Dict, Any
from dataclasses import dataclass, asdict
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import random
import argparse

# ============================================================================
# æ•°æ®æ¨¡å‹
# ============================================================================

@dataclass
class StandardizedEvent:
    """æ ‡å‡†åŒ–äº‹ä»¶æ¨¡å‹"""
    platform: str
    event_id: str
    original_id: str
    event_date: str
    event_time: str = None
    event_datetime: str = None
    title: str = ""
    content: str = None
    category: str = None
    importance: int = None
    country: str = None
    city: str = None
    stocks: List[str] = None
    concepts: List[Dict[str, Any]] = None
    themes: List[str] = None
    data_status: str = "ACTIVE"
    is_new: bool = False
    discovery_date: str = ""
    raw_data: Dict[str, Any] = None
    created_at: str = ""
    
    def __post_init__(self):
        if self.stocks is None:
            self.stocks = []
        if self.concepts is None:
            self.concepts = []
        if self.themes is None:
            self.themes = []
        if self.raw_data is None:
            self.raw_data = {}
        if not self.created_at:
            self.created_at = datetime.now().isoformat()
        if not self.discovery_date:
            self.discovery_date = datetime.now().strftime('%Y-%m-%d')
    
    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)

# ============================================================================
# å·¥å…·å‡½æ•°
# ============================================================================

def generate_sign(params_dict):
    """ç”Ÿæˆè´¢è”ç¤¾APIè¯·æ±‚çš„ç­¾å"""
    sorted_data = sorted(params_dict.items(), key=lambda item: item[0])
    query_string = urllib.parse.urlencode(sorted_data)
    sha1_hash = hashlib.sha1(query_string.encode('utf-8')).hexdigest()
    sign = hashlib.md5(sha1_hash.encode('utf-8')).hexdigest()
    return sign

def extract_json_from_jsonp(jsonp_text: str, callback_name: str) -> Dict[str, Any]:
    """ä»JSONPå“åº”ä¸­æå–JSONæ•°æ®"""
    try:
        pattern = f'{callback_name}\\((.+)\\);?$'
        match = re.search(pattern, jsonp_text.strip())
        if match:
            json_str = match.group(1)
            return json.loads(json_str)
    except:
        pass
    return {}

def load_platform_data(platform: str, data_path: str) -> List[StandardizedEvent]:
    """ä»txtæ–‡ä»¶åŠ è½½å¹³å°æ•°æ®"""
    file_path = os.path.join(data_path, f"{platform}.txt")
    
    if not os.path.exists(file_path):
        return []
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            json_content = f.read()
            data = json.loads(json_content)
        
        events = []
        for event_data in data.get('events', []):
            event = StandardizedEvent(**event_data)
            events.append(event)
        
        return events
    except Exception as e:
        print(f"åŠ è½½ {platform} æ•°æ®å¤±è´¥: {e}")
        return []

# ============================================================================
# å¹³å°æ•°æ®èŒƒå›´åŠ¨æ€æ£€æµ‹å™¨
# ============================================================================

class PlatformRangeDetector:
    """å¹³å°æ•°æ®èŒƒå›´åŠ¨æ€æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.range_cache = {}
    
    def get_platform_date_range(self, platform: str, start_date: str) -> str:
        """è·å–å¹³å°çš„å®é™…æ•°æ®èŒƒå›´"""
        cache_key = f"{platform}_{start_date}"
        
        if cache_key in self.range_cache:
            return self.range_cache[cache_key]
        
        if platform == "cls":
            max_date = self._detect_cls_max_date(start_date)
        elif platform == "jiuyangongshe":
            max_date = self._detect_jiuyan_max_date(start_date)
        elif platform == "tonghuashun":
            max_date = self._detect_tonghuashun_max_date(start_date)
        elif platform == "investing":
            max_date = self._detect_investing_max_date(start_date)
        elif platform == "eastmoney":
            max_date = self._detect_eastmoney_max_date(start_date)
        else:
            max_date = start_date
        
        self.range_cache[cache_key] = max_date
        return max_date
    
    def _detect_cls_max_date(self, start_date: str) -> str:
        """è´¢è”ç¤¾ï¼šé€šå¸¸æä¾›æœªæ¥6ä¸ªæœˆæ•°æ®"""
        test_date = (datetime.strptime(start_date, '%Y-%m-%d') + timedelta(days=180)).strftime('%Y-%m-%d')
        return test_date
    
    def _detect_jiuyan_max_date(self, start_date: str) -> str:
        """éŸ­ç ”å…¬ç¤¾ï¼šæµ‹è¯•åˆ°å¹´åº•"""
        return "2025-12-31"
    
    def _detect_tonghuashun_max_date(self, start_date: str) -> str:
        """åŒèŠ±é¡ºï¼šæµ‹è¯•åˆ°å¹´åº•"""
        return "2025-12-31"
    
    def _detect_investing_max_date(self, start_date: str) -> str:
        """è‹±ä¸ºè´¢æƒ…ï¼šæµ‹è¯•åˆ°æ˜å¹´åº•"""
        return "2025-12-31"
    
    def _detect_eastmoney_max_date(self, start_date: str) -> str:
        """ä¸œæ–¹è´¢å¯Œï¼šæµ‹è¯•åˆ°å¹´åº•"""
        return "2025-12-31"

# ============================================================================
# æœªæ¥æ•°æ®é‡‡é›†å™¨
# ============================================================================

class FutureDataCollector:
    """æœªæ¥æ•°æ®é‡‡é›†å™¨"""
    
    def __init__(self):
        self.base_path = "./data"
        self.current_path = os.path.join(self.base_path, "active", "current")
        self.range_detector = PlatformRangeDetector()
        self._ensure_directories()
    
    def _ensure_directories(self):
        """ç¡®ä¿ç›®å½•å­˜åœ¨"""
        os.makedirs(self.current_path, exist_ok=True)
    
    def collect_all_future_data(self) -> Dict[str, List[StandardizedEvent]]:
        """é‡‡é›†æ‰€æœ‰å¹³å°çš„æœªæ¥æ•°æ®ï¼ˆåŠ¨æ€èŒƒå›´ï¼‰"""
        today = datetime.now().strftime('%Y-%m-%d')
        
        print(f"ğŸ”® å¼€å§‹é‡‡é›†æœªæ¥æ•°æ®ï¼ˆä» {today} å¼€å§‹ï¼ŒåŠ¨æ€æ£€æµ‹æœ€è¿œæ—¥æœŸï¼‰")
        print("=" * 60)
        
        results = {}
        total_events = 0
        
        # è´¢è”ç¤¾
        print(f"\nğŸ“¡ æ­£åœ¨é‡‡é›† cls æœªæ¥æ•°æ®...")
        max_date = self.range_detector.get_platform_date_range('cls', today)
        print(f"   ğŸ“… cls æ•°æ®èŒƒå›´: {today} â†’ {max_date}")
        cls_events = self._collect_cls_future_dynamic(today, max_date)
        results['cls'] = cls_events
        total_events += len(cls_events)
        print(f"âœ… cls é‡‡é›†å®Œæˆï¼Œå…± {len(cls_events)} ä¸ªäº‹ä»¶")
        
        # éŸ­ç ”å…¬ç¤¾
        print(f"\nğŸ“¡ æ­£åœ¨é‡‡é›† jiuyangongshe æœªæ¥æ•°æ®...")
        max_date = self.range_detector.get_platform_date_range('jiuyangongshe', today)
        print(f"   ğŸ“… jiuyangongshe æ•°æ®èŒƒå›´: {today} â†’ {max_date}")
        jiuyan_events = self._collect_jiuyan_future_dynamic(today, max_date)
        results['jiuyangongshe'] = jiuyan_events
        total_events += len(jiuyan_events)
        print(f"âœ… jiuyangongshe é‡‡é›†å®Œæˆï¼Œå…± {len(jiuyan_events)} ä¸ªäº‹ä»¶")
        
        # åŒèŠ±é¡º
        print(f"\nğŸ“¡ æ­£åœ¨é‡‡é›† tonghuashun æœªæ¥æ•°æ®...")
        max_date = self.range_detector.get_platform_date_range('tonghuashun', today)
        print(f"   ğŸ“… tonghuashun æ•°æ®èŒƒå›´: {today} â†’ {max_date}")
        ths_events = self._collect_tonghuashun_future_dynamic(today, max_date)
        results['tonghuashun'] = ths_events
        total_events += len(ths_events)
        print(f"âœ… tonghuashun é‡‡é›†å®Œæˆï¼Œå…± {len(ths_events)} ä¸ªäº‹ä»¶")
        
        # è‹±ä¸ºè´¢æƒ…
        print(f"\nğŸ“¡ æ­£åœ¨é‡‡é›† investing æœªæ¥æ•°æ®...")
        max_date = self.range_detector.get_platform_date_range('investing', today)
        print(f"   ğŸ“… investing æ•°æ®èŒƒå›´: {today} â†’ {max_date}")
        inv_events = self._collect_investing_future_dynamic(today, max_date)
        results['investing'] = inv_events
        total_events += len(inv_events)
        print(f"âœ… investing é‡‡é›†å®Œæˆï¼Œå…± {len(inv_events)} ä¸ªäº‹ä»¶")
        
        # ä¸œæ–¹è´¢å¯Œ
        print(f"\nğŸ“¡ æ­£åœ¨é‡‡é›† eastmoney æœªæ¥æ•°æ®...")
        max_date = self.range_detector.get_platform_date_range('eastmoney', today)
        print(f"   ğŸ“… eastmoney æ•°æ®èŒƒå›´: {today} â†’ {max_date}")
        em_events = self._collect_eastmoney_future_dynamic(today, max_date)
        results['eastmoney'] = em_events
        total_events += len(em_events)
        print(f"âœ… eastmoney é‡‡é›†å®Œæˆï¼Œå…± {len(em_events)} ä¸ªäº‹ä»¶")
        
        print(f"\nğŸ“Š æ€»è®¡é‡‡é›† {total_events} ä¸ªæœªæ¥äº‹ä»¶")
        return results
    
    def _collect_cls_future_dynamic(self, start_date: str, end_date: str) -> List[StandardizedEvent]:
        """é‡‡é›†è´¢è”ç¤¾æœªæ¥æ•°æ®"""
        params = {
            "app": "CailianpressWeb",
            "flag": "0",  # ä»ä»Šä»¥åçš„æ•°æ®
            "os": "web",
            "sv": "8.4.6",
            "token": "65EOORtcq9jy9667Vw0qugGnpcm4vU894112432",
            "type": "0",
            "uid": "4112432"
        }
        params["sign"] = generate_sign(params)
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "application/json, text/plain, */*",
            "Referer": "https://www.cls.cn/investKalendar"
        }
        
        try:
            response = requests.get(
                "https://www.cls.cn/api/calendar/web/list", 
                params=params, headers=headers, timeout=10
            )
            
            if response.status_code == 200:
                data = response.json()
                events = []
                
                for day_data in data.get('data', []):
                    date = day_data.get('calendar_day')
                    
                    # åªè¦start_dateåŠä»¥åä¸”ä¸è¶…è¿‡end_dateçš„æ•°æ®
                    if not date or date < start_date or date > end_date:
                        continue
                    
                    for item in day_data.get('items', []):
                        event = StandardizedEvent(
                            platform="cls",
                            event_id=f"cls_{item.get('id')}_{date.replace('-', '')}_{item.get('type', 0)}",
                            original_id=str(item.get('id')),
                            event_date=date,
                            event_time=self._extract_time(item.get('calendar_time')),
                            event_datetime=item.get('calendar_time'),
                            title=item.get('title', ''),
                            category=self._get_cls_category(item.get('type')),
                            importance=self._get_cls_importance(item),
                            country=self._extract_cls_country(item),
                            raw_data=item
                        )
                        events.append(event)
                
                return events
            else:
                return []
        except Exception as e:
            print(f"è´¢è”ç¤¾APIè¯·æ±‚å¤±è´¥: {e}")
            return []
    
    def _collect_jiuyan_future_dynamic(self, start_date: str, end_date: str) -> List[StandardizedEvent]:
        """é‡‡é›†éŸ­ç ”å…¬ç¤¾æœªæ¥æ•°æ®"""
        all_events = []
        
        # æŒ‰æœˆé‡‡é›†åˆ°end_date
        start_dt = datetime.strptime(start_date, '%Y-%m-%d')
        end_dt = datetime.strptime(end_date, '%Y-%m-%d')
        
        current_dt = start_dt
        while current_dt <= end_dt:
            year, month = current_dt.year, current_dt.month
            
            try:
                events = self._get_jiuyan_month_data(year, month, start_date, end_date, is_future=True)
                all_events.extend(events)
                time.sleep(0.5)
            except Exception as e:
                print(f"éŸ­ç ”å…¬ç¤¾{year}å¹´{month}æœˆæ•°æ®é‡‡é›†å¤±è´¥: {e}")
            
            # ç§»åŠ¨åˆ°ä¸‹ä¸ªæœˆ
            if current_dt.month == 12:
                current_dt = current_dt.replace(year=current_dt.year + 1, month=1)
            else:
                current_dt = current_dt.replace(month=current_dt.month + 1)
        
        return all_events
    
    def _collect_tonghuashun_future_dynamic(self, start_date: str, end_date: str) -> List[StandardizedEvent]:
        """é‡‡é›†åŒèŠ±é¡ºæœªæ¥æ•°æ®"""
        all_events = []
        
        # æŒ‰æœˆé‡‡é›†åˆ°end_date
        start_dt = datetime.strptime(start_date, '%Y-%m-%d')
        end_dt = datetime.strptime(end_date, '%Y-%m-%d')
        
        current_dt = start_dt
        while current_dt <= end_dt:
            year, month = current_dt.year, current_dt.month
            
            try:
                events = self._get_tonghuashun_month_data(year, month, start_date, end_date, is_future=True)
                all_events.extend(events)
                time.sleep(0.5)
            except Exception as e:
                print(f"åŒèŠ±é¡º{year}å¹´{month}æœˆæ•°æ®é‡‡é›†å¤±è´¥: {e}")
            
            # ç§»åŠ¨åˆ°ä¸‹ä¸ªæœˆ
            if current_dt.month == 12:
                current_dt = current_dt.replace(year=current_dt.year + 1, month=1)
            else:
                current_dt = current_dt.replace(month=current_dt.month + 1)
        
        return all_events

    def _collect_investing_future_dynamic(self, start_date: str, end_date: str) -> List[StandardizedEvent]:
        """é‡‡é›†è‹±ä¸ºè´¢æƒ…æœªæ¥æ•°æ®ï¼ˆå¹¶å‘æŒ‰å¤©é‡‡é›†ï¼‰"""
        print(f"   ğŸ”„ è‹±ä¸ºè´¢æƒ…å¹¶å‘æŒ‰å¤©é‡‡é›†: {start_date} â†’ {end_date}")
        
        # ç”Ÿæˆæ—¥æœŸåˆ—è¡¨
        date_list = []
        current_date = datetime.strptime(start_date, '%Y-%m-%d')
        end_dt = datetime.strptime(end_date, '%Y-%m-%d')
        
        while current_date <= end_dt:
            date_list.append(current_date.strftime('%Y-%m-%d'))
            current_date += timedelta(days=1)
        
        print(f"   ğŸ“… æ€»å…±éœ€è¦é‡‡é›† {len(date_list)} å¤©çš„æ•°æ®")
        
        all_events = []
        max_workers = 5  # å¹¶å‘çº¿ç¨‹æ•°
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘é‡‡é›†
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_date = {
                executor.submit(self._request_investing_single_day_with_retry, date): date 
                for date in date_list
            }
            
            completed_count = 0
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_date):
                date = future_to_date[future]
                completed_count += 1
                
                try:
                    day_events = future.result()
                    if day_events:
                        all_events.extend(day_events)
                    
                    # æ¯50å¤©æˆ–æœ€åæ˜¾ç¤ºè¿›åº¦
                    if completed_count % 50 == 0 or completed_count == len(date_list):
                        print(f"      ğŸ“Š è¿›åº¦: {completed_count}/{len(date_list)} å¤©å®Œæˆï¼Œå·²è·å– {len(all_events)} ä¸ªäº‹ä»¶")
                        
                except Exception as e:
                    print(f"      âŒ {date} é‡‡é›†å¤±è´¥: {e}")
        
        print(f"   ğŸ“Š è‹±ä¸ºè´¢æƒ…æ€»è®¡: {len(all_events)} ä¸ªäº‹ä»¶ (å¹¶å‘é‡‡é›† {len(date_list)} å¤©)")
        return all_events

    def _request_investing_single_day_with_retry(self, date: str, max_retries: int = 3) -> List[StandardizedEvent]:
        """è¯·æ±‚è‹±ä¸ºè´¢æƒ…å•å¤©æ•°æ®ï¼ˆå¸¦é‡è¯•å’Œéšæœºå»¶è¿Ÿï¼‰"""
        for attempt in range(max_retries):
            try:
                # æ·»åŠ éšæœºå»¶è¿Ÿï¼Œé¿å…å¹¶å‘è¯·æ±‚è¿‡äºé›†ä¸­
                delay = random.uniform(0.2, 0.8)
                time.sleep(delay)
                
                return self._request_investing_single_day(date)
                
            except Exception as e:
                if attempt == max_retries - 1:
                    # æœ€åä¸€æ¬¡é‡è¯•å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
                    return []
                else:
                    # é‡è¯•å‰ç­‰å¾…
                    retry_delay = random.uniform(1.0, 2.0)
                    time.sleep(retry_delay)
        
        return []

    
    def _collect_eastmoney_future_dynamic(self, start_date: str, end_date: str) -> List[StandardizedEvent]:
        """é‡‡é›†ä¸œæ–¹è´¢å¯Œæœªæ¥æ•°æ®"""
        params = {
            "fromdate": start_date,
            "todate": end_date,
            "option": "xsap,xgsg,tfpxx,hsgg,nbjb,jjsj,hyhy,gddh"
        }
        
        headers = {
            "Host": "data.eastmoney.com",
            "Accept": "application/json, text/javascript, */*; q=0.01",
            "X-Requested-With": "XMLHttpRequest",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Referer": "https://data.eastmoney.com/dcrl/dashi.html"
        }
        
        try:
            response = requests.get(
                "https://data.eastmoney.com/dataapi/dcrl/dstx",
                params=params, headers=headers, timeout=10
            )
            
            if response.status_code == 200:
                data = response.json()
                events = []
                
                # å®‰å…¨è·å–æ•°æ®é•¿åº¦ï¼Œå¤„ç†Noneå€¼
                def safe_len(data_list):
                    return len(data_list) if data_list is not None else 0
                
                print(f"   ğŸ” ä¸œæ–¹è´¢å¯ŒåŸå§‹æ•°æ®ç»Ÿè®¡:")
                print(f"      ä¼‘å¸‚å®‰æ’(xsap): {safe_len(data.get('xsap'))} æ¡")
                print(f"      æ–°è‚¡ç”³è´­(xgsg): {safe_len(data.get('xgsg'))} æ¡")
                print(f"      åœå¤ç‰Œä¿¡æ¯(tfpxx): {safe_len(data.get('tfpxx'))} æ¡")
                print(f"      Aè‚¡å…¬å‘Š(hsgg): {safe_len(data.get('hsgg'))} æ¡")
                print(f"      å¹´æŠ¥å­£æŠ¥(nbjb): {safe_len(data.get('nbjb'))} æ¡")
                print(f"      ç»æµæ•°æ®(jjsj): {safe_len(data.get('jjsj'))} æ¡")
                print(f"      è¡Œä¸šä¼šè®®(hyhy): {safe_len(data.get('hyhy'))} æ¡")
                print(f"      è‚¡ä¸œå¤§ä¼š(gddh): {safe_len(data.get('gddh'))} æ¡")
                
                # å¤„ç†å„ç±»æ•°æ®
                events.extend(self._process_eastmoney_xsap(data.get('xsap') or [], start_date, end_date))
                events.extend(self._process_eastmoney_xgsg(data.get('xgsg') or [], start_date, end_date))
                events.extend(self._process_eastmoney_tfpxx(data.get('tfpxx') or [], start_date, end_date))
                events.extend(self._process_eastmoney_hsgg(data.get('hsgg') or [], start_date, end_date))
                events.extend(self._process_eastmoney_nbjb(data.get('nbjb') or [], start_date, end_date))
                events.extend(self._process_eastmoney_jjsj(data.get('jjsj') or [], start_date, end_date))
                events.extend(self._process_eastmoney_hyhy(data.get('hyhy') or [], start_date, end_date))
                events.extend(self._process_eastmoney_gddh(data.get('gddh') or [], start_date, end_date))
                
                print(f"   âœ… ä¸œæ–¹è´¢å¯Œè¿‡æ»¤åäº‹ä»¶: {len(events)} ä¸ª")
                return events
            else:
                return []
        except Exception as e:
            print(f"ä¸œæ–¹è´¢å¯ŒAPIè¯·æ±‚å¤±è´¥: {e}")
            return []
    
    # è´¢è”ç¤¾è¾…åŠ©æ–¹æ³•
    def _extract_time(self, datetime_str: str) -> str:
        if not datetime_str:
            return None
        try:
            dt = datetime.fromisoformat(datetime_str.replace('Z', '+00:00'))
            return dt.strftime('%H:%M:%S')
        except:
            return None
    
    def _get_cls_category(self, event_type: int) -> str:
        type_map = {1: 'ç»æµæ•°æ®', 2: 'äº‹ä»¶å…¬å‘Š', 3: 'å‡æ—¥'}
        return type_map.get(event_type, 'å…¶ä»–')
    
    def _get_cls_importance(self, item: Dict) -> int:
        if item.get('type') == 1 and item.get('economic'):
            return item.get('economic', {}).get('star', 3)
        elif item.get('type') == 2 and item.get('event'):
            return item.get('event', {}).get('star', 3)
        return 3
    
    def _extract_cls_country(self, item: Dict) -> str:
        if item.get('economic'):
            return item.get('economic', {}).get('country')
        elif item.get('event'):
            return item.get('event', {}).get('country')
        return None
    
    # éŸ­ç ”å…¬ç¤¾è¾…åŠ©æ–¹æ³•
    def _get_jiuyan_month_data(self, year: int, month: int, start_date: str, end_date: str, is_future: bool = False) -> List[StandardizedEvent]:
        """è·å–éŸ­ç ”å…¬ç¤¾æœˆåº¦æ•°æ®"""
        date_param = f"{year}-{month:02d}"
        
        headers = {
            'Content-Type': 'application/json',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'timestamp': str(int(time.time() * 1000)),
            'platform': '3',
            'token': '094cbed7fa79612f2ba4fcd34b191d99',
            'Origin': 'https://www.jiuyangongshe.com',
            'Referer': 'https://www.jiuyangongshe.com/'
        }
        
        payload = {"date": date_param}
        
        try:
            response = requests.post(
                "https://app.jiuyangongshe.com/jystock-app/api/v1/timeline/list",
                headers=headers, json=payload, timeout=10
            )
            
            if response.status_code == 200:
                month_data = response.json()
                events = []
                
                for day_data in month_data.get('data', []):
                    date = day_data.get('date')
                    
                    # è¿‡æ»¤æ—¥æœŸèŒƒå›´
                    if not date or date < start_date or date > end_date:
                        continue
                    
                    for item in day_data.get('list', []):
                        timeline = item.get('timeline', {})
                        event = StandardizedEvent(
                            platform="jiuyangongshe",
                            event_id=f"jygs_{item.get('article_id', '')}_{timeline.get('timeline_id', '')}_{date.replace('-', '')}",
                            original_id=item.get('article_id', ''),
                            event_date=date,
                            title=item.get('title', ''),
                            content=item.get('content', ''),
                            importance=max(1, min(5, 7 - timeline.get('grade', 6))),
                            country='ä¸­å›½',
                            themes=[theme.get('name', '') for theme in timeline.get('theme_list', [])],
                            raw_data=item
                        )
                        events.append(event)
                
                return events
            else:
                return []
        except Exception as e:
            print(f"éŸ­ç ”å…¬ç¤¾APIè¯·æ±‚å¤±è´¥: {e}")
            return []
    
    # åŒèŠ±é¡ºè¾…åŠ©æ–¹æ³•
    def _get_tonghuashun_month_data(self, year: int, month: int, start_date: str, end_date: str, is_future: bool = False) -> List[StandardizedEvent]:
        """è·å–åŒèŠ±é¡ºæœˆåº¦æ•°æ®"""
        date_param = f"{year}{month:02d}"
        
        params = {
            'callback': 'callback_dt',
            'type': 'data',
            'date': date_param
        }
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': 'https://stock.10jqka.com.cn/',
            'Accept': '*/*'
        }
        
        try:
            response = requests.get(
                "https://comment.10jqka.com.cn/tzrl/getTzrlData.php",
                params=params, headers=headers, timeout=10
            )
            
            if response.status_code == 200:
                json_data = extract_json_from_jsonp(response.text, 'callback_dt')
                events = []
                
                for day_data in json_data.get('data', []):
                    date = day_data.get('date')
                    
                    # è¿‡æ»¤æ—¥æœŸèŒƒå›´
                    if not date or date < start_date or date > end_date:
                        continue
                    
                    day_events = day_data.get('events', [])
                    concepts = day_data.get('concept', [])
                    
                    for i, event_data in enumerate(day_events):
                        if isinstance(event_data, list) and len(event_data) > 0:
                            title = event_data[0] if event_data[0] else ""
                            title_hash = hashlib.md5(title.encode('utf-8')).hexdigest()[:8]
                            concept_info = concepts[i] if i < len(concepts) else []
                            
                            event = StandardizedEvent(
                                platform="tonghuashun",
                                event_id=f"ths_{date.replace('-', '')}_{i}_{title_hash}",
                                original_id=f"{date}_{i}",
                                event_date=date,
                                title=title,
                                importance=3,
                                country='ä¸­å›½',
                                concepts=[{"code": c.get("code"), "name": c.get("name")} for c in concept_info] if concept_info else [],
                                raw_data={"event": event_data, "concept": concept_info}
                            )
                            events.append(event)
                
                return events
            else:
                return []
        except Exception as e:
            print(f"åŒèŠ±é¡ºAPIè¯·æ±‚å¤±è´¥: {e}")
            return []
    
    # è‹±ä¸ºè´¢æƒ…è¾…åŠ©æ–¹æ³•
    def _request_investing_single_day(self, date: str) -> List[StandardizedEvent]:
        """è¯·æ±‚è‹±ä¸ºè´¢æƒ…å•å¤©æ•°æ®"""
        countries = [37, 46, 6, 110, 14, 48, 32, 17, 10, 36, 43, 35, 72, 22, 41, 25, 12, 5, 4, 26, 178, 11, 39, 42]
        
        # æ„å»ºè¯·æ±‚ä½“ï¼ˆå•å¤©ï¼‰
        payload = ""
        for country in countries:
            payload += f"country%5B%5D={country}&"
        payload += f"dateFrom={date}&dateTo={date}&timeZone=28&timeFilter=timeRemain&currentTab=custom&limit_from=0"
        
        headers = {
            "Host": "cn.investing.com",
            "Connection": "keep-alive",
            "sec-ch-ua": '"Not A(Brand";v="99", "Google Chrome";v="121", "Chromium";v="121"',
            "sec-ch-ua-mobile": "?0",
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            "Content-Type": "application/x-www-form-urlencoded",
            "Accept": "*/*",
            "X-Requested-With": "XMLHttpRequest",
            "sec-ch-ua-platform": '"Windows"',
            "Origin": "https://cn.investing.com",
            "Sec-Fetch-Site": "same-origin",
            "Sec-Fetch-Mode": "cors",
            "Sec-Fetch-Dest": "empty",
            "Referer": "https://cn.investing.com/economic-calendar/",
            "Accept-Encoding": "gzip, deflate, br, zstd",
            "Accept-Language": "zh-CN,zh;q=0.9"
        }
        
        try:
            response = requests.post(
                "https://cn.investing.com/economic-calendar/Service/getCalendarFilteredData",
                headers=headers, data=payload, timeout=15
            )
            
            if response.status_code == 200:
                html_content = response.text
                
                # æ£€æŸ¥æ˜¯å¦è¿”å›ç©ºç»“æœ
                if len(html_content.strip()) < 100:
                    return []
                
                # å¤„ç†å¯èƒ½çš„JSONåµŒå¥—
                try:
                    json_data = json.loads(html_content)
                    if 'data' in json_data:
                        html_content = json_data['data']
                    elif isinstance(json_data, dict) and len(json_data) == 0:
                        return []
                except json.JSONDecodeError:
                    pass
                
                # å¤„ç†è½¬ä¹‰å­—ç¬¦
                if '\\u' in html_content or '\\"' in html_content:
                    html_content = html_content.encode().decode('unicode_escape')
                    html_content = html_content.replace('\\"', '"').replace('\\/', '/')
                
                # æ£€æŸ¥HTMLå†…å®¹æ˜¯å¦åŒ…å«äº‹ä»¶æ•°æ®
                if 'js-event-item' not in html_content and 'eventRowId_' not in html_content:
                    return []
                
                return self._parse_investing_html_simple(html_content, date)
            else:
                return []
        except Exception as e:
            return []
    
    def _parse_investing_html_simple(self, html_content: str, date: str) -> List[StandardizedEvent]:
        """è§£æè‹±ä¸ºè´¢æƒ…HTMLæ•°æ®"""
        events = []
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            event_rows = soup.find_all('tr', class_=re.compile(r'js-event-item'))
            
            if not event_rows:
                event_rows = soup.find_all('tr', id=re.compile(r'eventRowId_'))
            
            for row in event_rows:
                event_data = self._extract_investing_event_enhanced(row)
                if event_data and event_data.get('event_name'):
                    event_date = self._extract_date_from_datetime(event_data.get('datetime'))
                    
                    # åªè¦å½“å¤©çš„æ•°æ®
                    if event_date == date:
                        # æ„å»ºå†…å®¹å­—ç¬¦ä¸²ï¼ˆåŒ…å«æ•°å€¼ä¿¡æ¯ï¼‰
                        content_parts = []
                        if event_data.get('actual'):
                            content_parts.append(f"å®é™…å€¼: {event_data.get('actual')}")
                        if event_data.get('forecast'):
                            content_parts.append(f"é¢„æµ‹å€¼: {event_data.get('forecast')}")
                        if event_data.get('previous'):
                            content_parts.append(f"å‰å€¼: {event_data.get('previous')}")
                        
                        content = " | ".join(content_parts) if content_parts else None
                        
                        event = StandardizedEvent(
                            platform="investing",
                            event_id=f"inv_{event_data.get('event_attr_id', '')}_{event_data.get('datetime', '').replace('/', '').replace(' ', '').replace(':', '')}",
                            original_id=event_data.get('event_id', ''),
                            event_date=event_date,
                            event_time=event_data.get('time'),
                            event_datetime=event_data.get('datetime'),
                            title=event_data.get('event_name', ''),
                            content=content,
                            importance=event_data.get('importance', 1),
                            country=event_data.get('country'),
                            raw_data=event_data
                        )
                        events.append(event)
        except Exception as e:
            print(f"è‹±ä¸ºè´¢æƒ…HTMLè§£æå¤±è´¥: {e}")
        
        return events
    
    def _extract_investing_event_enhanced(self, row) -> Dict[str, Any]:
        """å¢å¼ºç‰ˆè‹±ä¸ºè´¢æƒ…äº‹ä»¶æ•°æ®æå–"""
        event = {}
        cells = row.find_all('td')
        
        if len(cells) < 4:
            return None
        
        event['event_id'] = row.get('id', '')
        event['event_attr_id'] = row.get('event_attr_ID', '')
        event['datetime'] = row.get('data-event-datetime', '')
        
        try:
            # ç¬¬1åˆ—ï¼šæ—¶é—´
            if len(cells) > 0:
                event['time'] = cells[0].get_text(strip=True)
            
            # ç¬¬2åˆ—ï¼šå›½å®¶
            if len(cells) > 1:
                flag_span = cells[1].find('span', class_=re.compile(r'ceFlags'))
                if flag_span:
                    event['country'] = flag_span.get('title', '')
            
            # ç¬¬3åˆ—ï¼šé‡è¦æ€§
            if len(cells) > 2:
                full_icons = cells[2].find_all('i', class_='grayFullBullishIcon')
                event['importance'] = len(full_icons) if full_icons else 1
            
            # ç¬¬4åˆ—ï¼šäº‹ä»¶åç§°
            if len(cells) > 3:
                event_cell = cells[3]
                link = event_cell.find('a')
                if link:
                    event['event_name'] = link.get_text(strip=True)
                else:
                    event['event_name'] = event_cell.get_text(strip=True)
            
            # ç¬¬5åˆ—ï¼šå®é™…å€¼ï¼ˆå¦‚æœæœ‰ï¼‰
            if len(cells) > 4:
                actual_value = cells[4].get_text(strip=True)
                if actual_value and actual_value not in ['--', '']:
                    event['actual'] = actual_value
            
            # ç¬¬6åˆ—ï¼šé¢„æµ‹å€¼ï¼ˆå¦‚æœæœ‰ï¼‰
            if len(cells) > 5:
                forecast_value = cells[5].get_text(strip=True)
                if forecast_value and forecast_value not in ['--', '']:
                    event['forecast'] = forecast_value
            
            # ç¬¬7åˆ—ï¼šå‰å€¼ï¼ˆå¦‚æœæœ‰ï¼‰
            if len(cells) > 6:
                previous_value = cells[6].get_text(strip=True)
                if previous_value and previous_value not in ['--', '']:
                    event['previous'] = previous_value
        
        except Exception as e:
            print(f"è‹±ä¸ºè´¢æƒ…äº‹ä»¶æå–å¤±è´¥: {e}")
            return None
        
        return event
    
    def _extract_date_from_datetime(self, datetime_str: str) -> str:
        """ä»datetimeå­—ç¬¦ä¸²ä¸­æå–æ—¥æœŸ"""
        if not datetime_str:
            return ""
        try:
            date_part = datetime_str.split(' ')[0]
            return date_part.replace('/', '-')
        except:
            return ""
    
    # ä¸œæ–¹è´¢å¯Œè¾…åŠ©æ–¹æ³•
    def _extract_date_fixed(self, date_str: str) -> str:
        """ä¿®å¤ç‰ˆæ—¥æœŸæå–"""
        if not date_str:
            return ""
        try:
            if ' ' in date_str:
                return date_str.split(' ')[0]
            return date_str
        except Exception as e:
            return ""
    
    def _extract_time_fixed(self, datetime_str: str) -> str:
        """ä¿®å¤ç‰ˆæ—¶é—´æå–"""
        if not datetime_str:
            return None
        try:
            if ' ' in datetime_str:
                parts = datetime_str.split(' ')
                if len(parts) > 1 and parts[1] != "00:00:00":
                    return parts[1]
        except:
            return None
    
    def _process_eastmoney_xsap(self, xsap_data: list, start_date: str, end_date: str) -> List[StandardizedEvent]:
        """å¤„ç†ä¼‘å¸‚å®‰æ’æ•°æ®"""
        events = []
        for item in xsap_data:
            if not item:
                continue
            item_date = self._extract_date_fixed(item.get('SDATE'))
            if item_date and start_date <= item_date <= end_date:
                event = StandardizedEvent(
                    platform="eastmoney",
                    event_id=f"em_xsap_{item_date.replace('-', '')}_{hash(item.get('HOLIDAY', ''))}",
                    original_id=f"xsap_{item.get('MKT', '')}_{item_date}",
                    event_date=item_date,
                    event_time=self._extract_time_fixed(item.get('SDATE')),
                    event_datetime=item.get('SDATE'),
                    title=f"{item.get('MKT', '')} - {item.get('HOLIDAY', '')}",
                    category='ä¼‘å¸‚å®‰æ’',
                    importance=2,
                    country='ä¸­å›½',
                    raw_data=item
                )
                events.append(event)
        return events
    
    def _process_eastmoney_xgsg(self, xgsg_data: list, start_date: str, end_date: str) -> List[StandardizedEvent]:
        """å¤„ç†æ–°è‚¡ç”³è´­æ•°æ®"""
        events = []
        for item in xgsg_data:
            if not item:
                continue
            item_date = self._extract_date_fixed(item.get('APPLY_DATE'))
            if item_date and start_date <= item_date <= end_date:
                event = StandardizedEvent(
                    platform="eastmoney",
                    event_id=f"em_xgsg_{item.get('SECURITY_CODE', '')}_{item_date.replace('-', '')}",
                    original_id=item.get('SECURITY_CODE', ''),
                    event_date=item_date,
                    title=f"{item.get('SECURITY_NAME_ABBR', '')}æ–°è‚¡ç”³è´­",
                    content=f"ç”³è´­ä»£ç : {item.get('APPLY_CODE', '')}, å‘è¡Œä»·: {item.get('ISSUE_PRICE', '')}, å‘è¡Œé‡: {item.get('ONLINE_ISSUE_LWR', '')}ä¸‡è‚¡",
                    category='æ–°è‚¡ç”³è´­',
                    importance=3,
                    country='ä¸­å›½',
                    stocks=[item.get('SECURITY_CODE', '')] if item.get('SECURITY_CODE') else [],
                    raw_data=item
                )
                events.append(event)
        return events
    
    def _process_eastmoney_tfpxx(self, tfpxx_data: list, start_date: str, end_date: str) -> List[StandardizedEvent]:
        """å¤„ç†åœå¤ç‰Œä¿¡æ¯"""
        events = []
        for item in tfpxx_data:
            if not item:
                continue
            item_date = self._extract_date_fixed(item.get('Date'))
            if item_date and start_date <= item_date <= end_date:
                stock_data = item.get('Data') or []
                for stock in stock_data:
                    if not stock:
                        continue
                    event = StandardizedEvent(
                        platform="eastmoney",
                        event_id=f"em_tfpxx_{item_date.replace('-', '')}_{stock.get('Scode', '')}",
                        original_id=stock.get('Scode', ''),
                        event_date=item_date,
                        title=f"{stock.get('Sname', '')}åœå¤ç‰Œ",
                        content=f"è‚¡ç¥¨ä»£ç : {stock.get('Scode', '')}, åœå¤ç‰ŒåŸå› : {stock.get('Reason', '')}",
                        category='åœå¤ç‰Œä¿¡æ¯',
                        importance=2,
                        country='ä¸­å›½',
                        stocks=[stock.get('Scode', '')] if stock.get('Scode') else [],
                        raw_data=stock
                    )
                    events.append(event)
        return events
    
    def _process_eastmoney_hsgg(self, hsgg_data: list, start_date: str, end_date: str) -> List[StandardizedEvent]:
        """å¤„ç†Aè‚¡å…¬å‘Š"""
        events = []
        for item in hsgg_data:
            if not item:
                continue
            item_date = self._extract_date_fixed(item.get('NOTICE_DATE'))
            if item_date and start_date <= item_date <= end_date:
                event = StandardizedEvent(
                    platform="eastmoney",
                    event_id=f"em_hsgg_{item.get('SECUCODE', '')}_{item_date.replace('-', '')}_{hash(item.get('TITLE', ''))}",
                    original_id=item.get('SECUCODE', ''),
                    event_date=item_date,
                    title=f"{item.get('SECURITY_NAME_ABBR', '')} - {item.get('TITLE', '')}",
                    content=item.get('TITLE', ''),
                    category='Aè‚¡å…¬å‘Š',
                    importance=3,
                    country='ä¸­å›½',
                    stocks=[item.get('SECURITY_CODE', '')] if item.get('SECURITY_CODE') else [],
                    raw_data=item
                )
                events.append(event)
        return events
    
    def _process_eastmoney_nbjb(self, nbjb_data: list, start_date: str, end_date: str) -> List[StandardizedEvent]:
        """å¤„ç†å¹´æŠ¥å­£æŠ¥"""
        events = []
        for item in nbjb_data:
            if not item:
                continue
            item_date = self._extract_date_fixed(item.get('REPORT_DATE'))
            if item_date and start_date <= item_date <= end_date:
                event = StandardizedEvent(
                    platform="eastmoney",
                    event_id=f"em_nbjb_{item.get('SECURITY_CODE', '')}_{item_date.replace('-', '')}",
                    original_id=item.get('SECURITY_CODE', ''),
                    event_date=item_date,
                    title=f"{item.get('SECURITY_NAME_ABBR', '')} {item.get('REPORT_TYPE', '')}",
                    content=f"æŠ¥å‘Šç±»å‹: {item.get('REPORT_TYPE', '')}, æŠ¥å‘ŠæœŸ: {item.get('REPORT_PERIOD', '')}",
                    category='å¹´æŠ¥å­£æŠ¥',
                    importance=4,
                    country='ä¸­å›½',
                    stocks=[item.get('SECURITY_CODE', '')] if item.get('SECURITY_CODE') else [],
                    raw_data=item
                )
                events.append(event)
        return events
    
    def _process_eastmoney_jjsj(self, jjsj_data: list, start_date: str, end_date: str) -> List[StandardizedEvent]:
        """å¤„ç†ç»æµæ•°æ®"""
        events = []
        for item in jjsj_data:
            if not item:
                continue
            item_date = self._extract_date_fixed(item.get('Date'))
            if item_date and start_date <= item_date <= end_date:
                data_items = item.get('Data') or []
                for data_item in data_items:
                    if not data_item:
                        continue
                    event = StandardizedEvent(
                        platform="eastmoney",
                        event_id=f"em_jjsj_{item.get('Date', '').replace('-', '').replace(' ', '').replace(':', '')}_{hash(data_item.get('Name', ''))}",
                        original_id=f"{item.get('Date')}_{data_item.get('Name')}",
                        event_date=item_date,
                        event_time=self._extract_time_fixed(item.get('Date')),
                        event_datetime=item.get('Date'),
                        title=data_item.get('Name', ''),
                        category='ç»æµæ•°æ®',
                        importance=4,
                        country=item.get('City', ''),
                        raw_data=data_item
                    )
                    events.append(event)
        return events
    
    def _process_eastmoney_hyhy(self, hyhy_data: list, start_date: str, end_date: str) -> List[StandardizedEvent]:
        """å¤„ç†è¡Œä¸šä¼šè®®"""
        events = []
        for item in hyhy_data:
            if not item:
                continue
            start_event_date = self._extract_date_fixed(item.get('START_DATE'))
            if start_event_date and start_date <= start_event_date <= end_date:
                event = StandardizedEvent(
                    platform="eastmoney",
                    event_id=f"em_hyhy_{item.get('FE_CODE', '')}",
                    original_id=item.get('FE_CODE', ''),
                    event_date=start_event_date,
                    title=item.get('FE_NAME', ''),
                    content=item.get('CONTENT', ''),
                    category='è¡Œä¸šä¼šè®®',
                    importance=3,
                    country='ä¸­å›½',
                    city=item.get('CITY'),
                    raw_data=item
                )
                events.append(event)
        return events
    
    def _process_eastmoney_gddh(self, gddh_data: list, start_date: str, end_date: str) -> List[StandardizedEvent]:
        """å¤„ç†è‚¡ä¸œå¤§ä¼š"""
        events = []
        for item in gddh_data:
            if not item:
                continue
            item_date = self._extract_date_fixed(item.get('MEETING_DATE'))
            if item_date and start_date <= item_date <= end_date:
                event = StandardizedEvent(
                    platform="eastmoney",
                    event_id=f"em_gddh_{item.get('SECURITY_CODE', '')}_{item_date.replace('-', '')}",
                    original_id=item.get('SECURITY_CODE', ''),
                    event_date=item_date,
                    event_time=self._extract_time_fixed(item.get('MEETING_DATE')),
                    event_datetime=item.get('MEETING_DATE'),
                    title=f"{item.get('SECURITY_NAME_ABBR', '')}è‚¡ä¸œå¤§ä¼š",
                    content=f"ä¼šè®®ç±»å‹: {item.get('MEETING_TYPE', '')}, åœ°ç‚¹: {item.get('MEETING_PLACE', '')}",
                    category='è‚¡ä¸œå¤§ä¼š',
                    importance=3,
                    country='ä¸­å›½',
                    city=item.get('MEETING_PLACE'),
                    stocks=[item.get('SECURITY_CODE', '')] if item.get('SECURITY_CODE') else [],
                    raw_data=item
                )
                events.append(event)
        return events

# ============================================================================
# æ•°æ®å­˜å‚¨ç®¡ç†
# ============================================================================

class DataStorage:
    """æ•°æ®å­˜å‚¨ç®¡ç†"""
    
    def __init__(self):
        self.base_path = "./data"
        self.current_path = os.path.join(self.base_path, "active", "current")
        self.previous_path = os.path.join(self.base_path, "active", "previous")
        self.archived_path = os.path.join(self.base_path, "archived")
        self._ensure_directories()
    
    def _ensure_directories(self):
        """ç¡®ä¿ç›®å½•å­˜åœ¨"""
        os.makedirs(self.current_path, exist_ok=True)
        os.makedirs(self.previous_path, exist_ok=True)
    
    def save_platform_data(self, platform: str, events: List[StandardizedEvent]):
        """ä¿å­˜å¹³å°æ•°æ®ä¸ºtxtæ–‡ä»¶"""
        file_path = os.path.join(self.current_path, f"{platform}.txt")
        
        data = {
            "platform": platform,
            "total_events": len(events),
            "data_status": "ACTIVE",
            "date_type": "FUTURE",
            "last_updated": datetime.now().isoformat(),
            "immutable": False,
            "events": [event.to_dict() for event in events]
        }
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(json.dumps(data, ensure_ascii=False, indent=2))
        
        print(f"ğŸ’¾ {platform} æ•°æ®å·²ä¿å­˜åˆ° {file_path}")

    def save_all_data(self, all_data: Dict[str, List[StandardizedEvent]]):
        """ä¿å­˜æ‰€æœ‰å¹³å°æ•°æ®"""
        for platform, events in all_data.items():
            self.save_platform_data(platform, events)
        
        # ç”Ÿæˆæ±‡æ€»ä¿¡æ¯
        self._generate_summary(all_data)
    
    def _generate_summary(self, all_data: Dict[str, List[StandardizedEvent]]):
        """ç”Ÿæˆæ•°æ®æ±‡æ€»ä¿¡æ¯"""
        summary = {
            "collection_type": "ACTIVE",
            "collection_time": datetime.now().isoformat(),
            "platforms": {},
            "total_events": 0,
            "date_range": {
                "start": None,
                "end": None
            }
        }
        
        all_dates = []
        
        for platform, events in all_data.items():
            platform_dates = [event.event_date for event in events if event.event_date]
            all_dates.extend(platform_dates)
            
            summary["platforms"][platform] = {
                "event_count": len(events),
                "date_range": {
                    "start": min(platform_dates) if platform_dates else None,
                    "end": max(platform_dates) if platform_dates else None
                }
            }
            summary["total_events"] += len(events)
        
        if all_dates:
            summary["date_range"]["start"] = min(all_dates)
            summary["date_range"]["end"] = max(all_dates)
        
        # ä¿å­˜æ±‡æ€»ä¿¡æ¯ä¸ºtxtæ–‡ä»¶
        summary_path = os.path.join(self.current_path, "metadata.txt")
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(json.dumps(summary, ensure_ascii=False, indent=2))
        
        print(f"ğŸ“Š æ•°æ®æ±‡æ€»ä¿¡æ¯å·²ä¿å­˜åˆ° {summary_path}")

# ============================================================================
# äº‹ä»¶å˜æ›´æ£€æµ‹å¼•æ“
# ============================================================================

class ChangeDetectionEngine:
    """äº‹ä»¶å˜æ›´æ£€æµ‹å¼•æ“"""
    
    def __init__(self):
        self.base_path = "./data"
        self.current_path = os.path.join(self.base_path, "active", "current")
        self.previous_path = os.path.join(self.base_path, "active", "previous")
    
    def detect_all_changes_with_new_data(self, new_data: Dict[str, List[StandardizedEvent]]) -> Dict[str, Dict[str, List[StandardizedEvent]]]:
        """ä½¿ç”¨æ–°é‡‡é›†çš„æ•°æ®è¿›è¡Œå˜æ›´æ£€æµ‹"""
        platforms = ['cls', 'jiuyangongshe', 'tonghuashun', 'investing', 'eastmoney']
        all_changes = {}
        
        print("ğŸ” å¼€å§‹æ£€æµ‹äº‹ä»¶å˜æ›´...")
        print("=" * 50)
        
        total_new = 0
        total_updated = 0
        total_cancelled = 0
        
        for platform in platforms:
            print(f"\nğŸ“Š æ£€æµ‹ {platform} å¹³å°å˜æ›´...")
            
            try:
                # åŠ è½½previousæ•°æ®
                previous_events = load_platform_data(platform, self.previous_path)
                
                # ä½¿ç”¨æ–°é‡‡é›†çš„æ•°æ®ä½œä¸ºcurrent
                current_events = new_data.get(platform, [])
                
                # æ£€æµ‹å˜æ›´
                changes = self._detect_platform_changes(platform, previous_events, current_events)
                all_changes[platform] = changes
                
                # ç»Ÿè®¡
                new_count = len(changes['new_events'])
                updated_count = len(changes['updated_events'])
                cancelled_count = len(changes['cancelled_events'])
                
                total_new += new_count
                total_updated += updated_count
                total_cancelled += cancelled_count
                
                print(f"   æ–°å¢: {new_count}, æ›´æ–°: {updated_count}, å–æ¶ˆ: {cancelled_count}")
                
                # å¦‚æœæœ‰å˜æ›´ï¼Œæ ‡è®°æ–°æ•°æ®ä¸­çš„äº‹ä»¶
                if new_count > 0 or updated_count > 0:
                    self._mark_changes_in_new_data(platform, new_data[platform], changes)
                
            except Exception as e:
                print(f"âŒ {platform} å˜æ›´æ£€æµ‹å¤±è´¥: {e}")
                all_changes[platform] = {
                    'new_events': [],
                    'updated_events': [],
                    'cancelled_events': []
                }
        
        # ç”Ÿæˆå˜æ›´æŠ¥å‘Š
        self._generate_change_report(all_changes, total_new, total_updated, total_cancelled)
        
        return all_changes
    
    def _detect_platform_changes(self, platform: str, previous_events: List[StandardizedEvent], 
                                current_events: List[StandardizedEvent]) -> Dict[str, List[StandardizedEvent]]:
        """æ£€æµ‹å•ä¸ªå¹³å°çš„å˜æ›´"""
        
        # æ„å»ºäº‹ä»¶æ˜ å°„
        prev_map = {self._generate_event_key(event): event for event in previous_events}
        curr_map = {self._generate_event_key(event): event for event in current_events}
        
        new_events = []
        updated_events = []
        cancelled_events = []
        
        today = datetime.now().strftime('%Y-%m-%d')
        
        # æ£€æµ‹æ–°å¢å’Œæ›´æ–°äº‹ä»¶
        for key, curr_event in curr_map.items():
            if curr_event.event_date >= today:
                if key not in prev_map:
                    # æ–°å¢äº‹ä»¶
                    new_events.append(curr_event)
                else:
                    # æ£€æŸ¥æ˜¯å¦æœ‰å†…å®¹æ›´æ–°
                    prev_event = prev_map[key]
                    if self._has_content_changed(prev_event, curr_event):
                        updated_events.append(curr_event)
        
        # æ£€æµ‹å–æ¶ˆäº‹ä»¶
        for key, prev_event in prev_map.items():
            if prev_event.event_date >= today and key not in curr_map:
                cancelled_events.append(prev_event)
        
        return {
            'new_events': new_events,
            'updated_events': updated_events,
            'cancelled_events': cancelled_events
        }
    
    def _mark_changes_in_new_data(self, platform: str, current_events: List[StandardizedEvent], 
                                 changes: Dict[str, List[StandardizedEvent]]):
        """åœ¨æ–°æ•°æ®ä¸­æ ‡è®°å˜æ›´äº‹ä»¶"""
        today = datetime.now().strftime('%Y-%m-%d')
        
        # æ ‡è®°æ–°å¢äº‹ä»¶
        new_event_keys = {self._generate_event_key(event) for event in changes['new_events']}
        
        # æ ‡è®°æ›´æ–°äº‹ä»¶
        updated_event_keys = {self._generate_event_key(event) for event in changes['updated_events']}
        
        for event in current_events:
            event_key = self._generate_event_key(event)
            
            if event_key in new_event_keys:
                event.is_new = True
                event.discovery_date = today
            elif event_key in updated_event_keys:
                event.discovery_date = today
    
    def _generate_event_key(self, event: StandardizedEvent) -> str:
        """ç”Ÿæˆäº‹ä»¶å”¯ä¸€æ ‡è¯†"""
        if event.platform == "cls":
            return f"{event.original_id}_{event.event_date}_{event.category}"
        elif event.platform == "jiuyangongshe":
            return f"{event.original_id}_{event.event_date}"
        elif event.platform == "tonghuashun":
            return f"{event.event_date}_{hashlib.md5(event.title.encode()).hexdigest()[:8]}"
        elif event.platform == "investing":
            return f"{event.raw_data.get('event_attr_id', '')}_{event.event_date}_{event.event_time}"
        elif event.platform == "eastmoney":
            return f"{event.original_id}_{event.event_date}"
        else:
            return f"{event.event_id}"
    
    def _has_content_changed(self, prev_event: StandardizedEvent, curr_event: StandardizedEvent) -> bool:
        """æ£€æµ‹å†…å®¹æ˜¯å¦å˜æ›´"""
        return (
            prev_event.title != curr_event.title or
            prev_event.event_datetime != curr_event.event_datetime or
            prev_event.importance != curr_event.importance or
            prev_event.content != curr_event.content or
            prev_event.country != curr_event.country
        )
    
    def _generate_change_report(self, all_changes: Dict[str, Dict[str, List[StandardizedEvent]]], 
                              total_new: int, total_updated: int, total_cancelled: int):
        """ç”Ÿæˆå˜æ›´æŠ¥å‘Š"""
        today = datetime.now().strftime('%Y-%m-%d')
        
        report = {
            "detection_time": datetime.now().isoformat(),
            "summary": {
                "total_new": total_new,
                "total_updated": total_updated,
                "total_cancelled": total_cancelled
            },
            "platforms": {}
        }
        
        # æ”¶é›†æ‰€æœ‰æ–°å¢äº‹ä»¶ç”¨äºæ’åº
        all_new_events = []
        
        for platform, changes in all_changes.items():
            new_count = len(changes['new_events'])
            updated_count = len(changes['updated_events'])
            cancelled_count = len(changes['cancelled_events'])
            
            report["platforms"][platform] = {
                "new_events": new_count,
                "updated_events": updated_count,
                "cancelled_events": cancelled_count,
                "sample_new_titles": [event.title[:50] for event in changes['new_events'][:3]]
            }
            
            all_new_events.extend(changes['new_events'])
        
        # æŒ‰é‡è¦æ€§æ’åºæ–°å¢äº‹ä»¶
        all_new_events.sort(key=lambda x: x.importance or 0, reverse=True)
        report["top_new_events"] = [
            {
                "platform": event.platform,
                "date": event.event_date,
                "title": event.title,
                "importance": event.importance,
                "country": event.country
            }
            for event in all_new_events[:10]
        ]
        
        # ä¿å­˜å˜æ›´æŠ¥å‘Š
        report_path = os.path.join(self.current_path, f"change_report_{today}.txt")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(json.dumps(report, ensure_ascii=False, indent=2))
        
        print(f"\nğŸ“‹ å˜æ›´æŠ¥å‘Šå·²ä¿å­˜åˆ° {report_path}")
        
        # æ‰“å°ç»Ÿè®¡
        print(f"\nğŸ“ˆ ä»Šæ—¥å˜æ›´ç»Ÿè®¡:")
        print(f"   æ–°å¢äº‹ä»¶: {total_new} ä¸ª")
        print(f"   æ›´æ–°äº‹ä»¶: {total_updated} ä¸ª")
        print(f"   å–æ¶ˆäº‹ä»¶: {total_cancelled} ä¸ª")

# ============================================================================
# æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†
# ============================================================================

class DataLifecycleManager:
    """æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨"""
    
    def __init__(self):
        self.base_path = "./data"
        self.current_path = os.path.join(self.base_path, "active", "current")
        self.previous_path = os.path.join(self.base_path, "active", "previous")
        self.archived_path = os.path.join(self.base_path, "archived")
    
    def archive_specific_date_data(self, target_date: str):
        """å½’æ¡£æŒ‡å®šæ—¥æœŸçš„æ•°æ®"""
        print(f"ğŸ“¦ å½’æ¡£ {target_date} çš„æ•°æ®...")
        
        target_dt = datetime.strptime(target_date, '%Y-%m-%d')
        year, month = target_dt.year, target_dt.month
        
        platforms = ['cls', 'jiuyangongshe', 'tonghuashun', 'investing', 'eastmoney']
        archived_count = 0
        
        for platform in platforms:
            # ä»currentä¸­æå–ç›®æ ‡æ—¥æœŸçš„æ•°æ®
            current_events = load_platform_data(platform, self.current_path)
            target_date_events = [
                event for event in current_events 
                if event.event_date == target_date
            ]
            
            if target_date_events:
                # æ ‡è®°ä¸ºå·²å½’æ¡£
                for event in target_date_events:
                    event.data_status = "ARCHIVED"
                
                # ä¿å­˜åˆ°å½’æ¡£ç›®å½•
                self._append_to_archive(platform, year, month, target_date_events)
                archived_count += len(target_date_events)
                
                print(f"   ğŸ“¦ {platform}: {len(target_date_events)} ä¸ªäº‹ä»¶å·²å½’æ¡£")
        
        print(f"âœ… {target_date} å…±å½’æ¡£ {archived_count} ä¸ªäº‹ä»¶")
    
    def rotate_future_data_only(self, archive_date: str):
        """ç²¾ç¡®è½®è½¬ï¼šåªç§»åŠ¨æœªæ¥æ•°æ®åˆ°previous"""
        print(f"ğŸ”„ è½®è½¬æœªæ¥æ•°æ®ï¼ˆæ’é™¤ {archive_date}ï¼‰...")
        
        # æ¸…ç©ºå¹¶é‡å»ºpreviousç›®å½•
        if os.path.exists(self.previous_path):
            shutil.rmtree(self.previous_path)
        os.makedirs(self.previous_path, exist_ok=True)
        
        platforms = ['cls', 'jiuyangongshe', 'tonghuashun', 'investing', 'eastmoney']
        
        for platform in platforms:
            current_events = load_platform_data(platform, self.current_path)
            
            # ç­›é€‰ï¼šåªè¦archive_dateä¹‹åçš„æ•°æ®
            future_events = [
                event for event in current_events 
                if event.event_date > archive_date
            ]
            
            print(f"   ğŸ“‹ {platform}: {len(future_events)} ä¸ªæœªæ¥äº‹ä»¶ç§»è‡³previous")
            
            # ä¿å­˜åˆ°previous
            if future_events:
                self._save_platform_data_to_path(platform, future_events, self.previous_path)
    
    def _save_platform_data_to_path(self, platform: str, events: List[StandardizedEvent], target_path: str):
        """ä¿å­˜å¹³å°æ•°æ®åˆ°æŒ‡å®šè·¯å¾„"""
        file_path = os.path.join(target_path, f"{platform}.txt")
        
        data = {
            "platform": platform,
            "total_events": len(events),
            "data_status": "ACTIVE",
            "date_type": "FUTURE",
            "last_updated": datetime.now().isoformat(),
            "immutable": False,
            "events": [event.to_dict() for event in events]
        }
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(json.dumps(data, ensure_ascii=False, indent=2))
    
    def _append_to_archive(self, platform: str, year: int, month: int, events: List[StandardizedEvent]):
        """ä¿å­˜æ•°æ®åˆ°å½’æ¡£ç›®å½•"""
        month_path = os.path.join(self.archived_path, str(year), f"{month:02d}æœˆ")
        os.makedirs(month_path, exist_ok=True)
        
        file_path = os.path.join(month_path, f"{platform}.txt")
        
        # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆå¹¶æ•°æ®
        existing_events = []
        if os.path.exists(file_path):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    existing_data = json.loads(f.read())
                    for event_data in existing_data.get('events', []):
                        existing_events.append(StandardizedEvent(**event_data))
            except:
                pass
        
        # åˆå¹¶å¹¶å»é‡
        all_events = existing_events + events
        seen_ids = set()
        unique_events = []
        for event in all_events:
            if event.event_id not in seen_ids:
                unique_events.append(event)
                seen_ids.add(event.event_id)
        
        # æ„å»ºæ•°æ®ç»“æ„
        data = {
            "platform": platform,
            "year": year,
            "month": month,
            "total_events": len(unique_events),
            "data_status": "ARCHIVED",
            "date_type": "HISTORICAL",
            "last_update": datetime.now().isoformat(),
            "immutable": True,
            "events": [event.to_dict() for event in unique_events]
        }
        
        # ä¿å­˜ä¸ºtxtæ–‡ä»¶
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(json.dumps(data, ensure_ascii=False, indent=2))

# ============================================================================
# æ—¥å¸¸ä»»åŠ¡è°ƒåº¦å™¨
# ============================================================================

class DailyTaskScheduler:
    """æ—¥å¸¸ä»»åŠ¡è°ƒåº¦å™¨"""
    
    def __init__(self):
        self.collector = FutureDataCollector()
        self.storage = DataStorage()
        self.change_detector = ChangeDetectionEngine()
        self.lifecycle_manager = DataLifecycleManager()
    
    def run_first_time(self):
        """é¦–æ¬¡è¿è¡Œï¼šåªé‡‡é›†å½“å¤©åŠæœªæ¥æ•°æ®"""
        today = datetime.now().strftime('%Y-%m-%d')
        
        print(f"ğŸš€ é¦–æ¬¡è¿è¡Œæ¨¡å¼ï¼šé‡‡é›† {today} è‡³å„å¹³å°æœ€è¿œæ—¥æœŸ")
        print("=" * 60)
        
        # æ£€æŸ¥æ˜¯å¦çœŸçš„æ˜¯é¦–æ¬¡è¿è¡Œ
        if not self._is_first_run():
            print("âŒ æ£€æµ‹åˆ°å·²æœ‰æ´»è·ƒæ•°æ®ï¼Œè¯·ä½¿ç”¨ --daily æ¨¡å¼")
            return False
        
        try:
            # é‡‡é›†æœªæ¥æ•°æ®ï¼ˆåŒ…å«ä»Šå¤©ï¼‰
            future_data = self.collector.collect_all_future_data()
            self.storage.save_all_data(future_data)
            
            # åˆ›å»ºé¦–æ¬¡è¿è¡Œæ ‡è®°
            self._create_first_run_marker()
            
            print("âœ… é¦–æ¬¡è¿è¡Œå®Œæˆï¼Œæ˜å¤©å¼€å§‹ä½¿ç”¨ --daily æ¨¡å¼")
            return True
            
        except Exception as e:
            print(f"âŒ é¦–æ¬¡è¿è¡Œå¤±è´¥: {e}")
            return False
    
    def run_daily_update(self):
        """æ‰§è¡Œæ¯æ—¥æ›´æ–°æµç¨‹"""
        today = datetime.now().strftime('%Y-%m-%d')
        yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
        
        print(f"ğŸ”„ æ—¥å¸¸æ›´æ–°æ¨¡å¼ï¼šå¤„ç† {yesterday} å½’æ¡£ï¼Œé‡‡é›† {today} è‡³æœ€è¿œæ—¥æœŸ")
        print("=" * 60)
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºé¦–æ¬¡è¿è¡Œ
        if self._is_first_run():
            print("âŒ æ£€æµ‹åˆ°é¦–æ¬¡è¿è¡ŒçŠ¶æ€ï¼Œè¯·å…ˆä½¿ç”¨ --first-run æ¨¡å¼")
            return False
        
        try:
            # ç¬¬1æ­¥ï¼šå½’æ¡£æ˜¨å¤©çš„æ•°æ®
            print(f"\nğŸ“¦ ç¬¬1æ­¥ï¼šå½’æ¡£ {yesterday} çš„æ•°æ®")
            self.lifecycle_manager.archive_specific_date_data(yesterday)
            
            # ç¬¬2æ­¥ï¼šæ•°æ®è½®è½¬ï¼ˆåªè½®è½¬æœªæ¥æ•°æ®ï¼‰
            print(f"\nğŸ”„ ç¬¬2æ­¥ï¼šè½®è½¬æœªæ¥æ•°æ®åˆ° previous")
            self.lifecycle_manager.rotate_future_data_only(yesterday)
            
            # ç¬¬3æ­¥ï¼šé‡‡é›†æ–°çš„æœªæ¥æ•°æ®
            print(f"\nğŸ“¡ ç¬¬3æ­¥ï¼šé‡‡é›† {today} è‡³æœ€è¿œæ—¥æœŸçš„æ•°æ®")
            future_data = self.collector.collect_all_future_data()
            
            # ç¬¬4æ­¥ï¼šæ£€æµ‹äº‹ä»¶å˜æ›´
            print(f"\nğŸ” ç¬¬4æ­¥ï¼šæ£€æµ‹äº‹ä»¶å˜æ›´")
            changes = self.change_detector.detect_all_changes_with_new_data(future_data)
            
            # ç¬¬5æ­¥ï¼šä¿å­˜æ–°æ•°æ®åˆ°current
            print(f"\nğŸ’¾ ç¬¬5æ­¥ï¼šä¿å­˜æ–°æ•°æ®")
            self.storage.save_all_data(future_data)
            
            print("\nâœ… æ—¥å¸¸æ›´æ–°æµç¨‹å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ æ—¥å¸¸æ›´æ–°æµç¨‹å¤±è´¥: {e}")
            return False
    
    def _is_first_run(self):
        """æ£€æŸ¥æ˜¯å¦ä¸ºé¦–æ¬¡è¿è¡Œ"""
        # æ£€æŸ¥currentç›®å½•æ˜¯å¦ä¸ºç©ºæˆ–ä¸å­˜åœ¨
        if not os.path.exists(self.storage.current_path):
            return True
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¹³å°æ•°æ®æ–‡ä»¶
        platform_files = [f for f in os.listdir(self.storage.current_path) 
                          if f.endswith('.txt') and f not in ['metadata.txt', 'first_run_marker.txt']]
        return len(platform_files) == 0
    
    def _create_first_run_marker(self):
        """åˆ›å»ºé¦–æ¬¡è¿è¡Œæ ‡è®°"""
        marker_path = os.path.join(self.storage.current_path, "first_run_marker.txt")
        marker_data = {
            "first_run_date": datetime.now().strftime('%Y-%m-%d'),
            "first_run_time": datetime.now().isoformat(),
            "status": "completed"
        }
        
        with open(marker_path, 'w', encoding='utf-8') as f:
            f.write(json.dumps(marker_data, ensure_ascii=False, indent=2))

# ============================================================================
# æ•°æ®å¯¼å‡ºåŠŸèƒ½
# ============================================================================

def export_data_for_web():
    """å¯¼å‡ºæ•°æ®ä¾›ç½‘é¡µä½¿ç”¨"""
    print("ğŸ“¤ å¯¼å‡ºæ•°æ®ä¾›ç½‘é¡µä½¿ç”¨...")
    
    # åˆ›å»ºå¯¼å‡ºç›®å½•
    export_dir = "./docs/data"
    os.makedirs(export_dir, exist_ok=True)
    
    # è·å–å½“å‰æ—¥æœŸ
    today = datetime.now().strftime('%Y-%m-%d')
    
    # 1. å¯¼å‡ºæ•°æ®æ‘˜è¦
    export_summary(export_dir)
    
    # 2. å¯¼å‡ºæœ€æ–°äº‹ä»¶
    export_latest_events(export_dir, today)
    
    # 3. å¯¼å‡ºæ—¥å†æ•°æ®
    export_calendar_data(export_dir, today)
    
    print("âœ… æ•°æ®å¯¼å‡ºå®Œæˆ")

def export_summary(export_dir):
    """å¯¼å‡ºæ•°æ®æ‘˜è¦"""
    current_path = "./data/active/current"
    summary_path = os.path.join(current_path, "metadata.txt")
    
    if os.path.exists(summary_path):
        with open(summary_path, 'r', encoding='utf-8') as f:
            summary_data = json.loads(f.read())
        
        # æ·»åŠ æ›´å¤šç»Ÿè®¡ä¿¡æ¯
        platforms = ['cls', 'jiuyangongshe', 'tonghuashun', 'investing', 'eastmoney']
        platform_names = {
            'cls': 'è´¢è”ç¤¾',
            'jiuyangongshe': 'éŸ­ç ”å…¬ç¤¾',
            'tonghuashun': 'åŒèŠ±é¡º',
            'investing': 'è‹±ä¸ºè´¢æƒ…',
            'eastmoney': 'ä¸œæ–¹è´¢å¯Œ'
        }
        
        # ç»Ÿè®¡æ–°å¢äº‹ä»¶
        total_new = 0
        platform_new = {}
        
        for platform in platforms:
            events = load_platform_data(platform, current_path)
            new_events = [e for e in events if e.is_new]
            total_new += len(new_events)
            platform_new[platform] = len(new_events)
        
        summary_data['new_events'] = {
            'total': total_new,
            'platforms': platform_new
        }
        
        # æ·»åŠ å¹³å°ä¸­æ–‡åç§°
        for platform in platforms:
            if platform in summary_data['platforms']:
                summary_data['platforms'][platform]['name'] = platform_names.get(platform, platform)
        
        # ä¿å­˜åˆ°å¯¼å‡ºç›®å½•
        with open(os.path.join(export_dir, "summary.json"), 'w', encoding='utf-8') as f:
            f.write(json.dumps(summary_data, ensure_ascii=False, indent=2))
        
        print(f"   ğŸ“Š æ•°æ®æ‘˜è¦å·²å¯¼å‡º")

def export_latest_events(export_dir, today):
    """å¯¼å‡ºæœ€æ–°äº‹ä»¶"""
    current_path = "./data/active/current"
    platforms = ['cls', 'jiuyangongshe', 'tonghuashun', 'investing', 'eastmoney']
    
    # æ”¶é›†æ‰€æœ‰æ–°å¢äº‹ä»¶
    all_new_events = []
    for platform in platforms:
        events = load_platform_data(platform, current_path)
        new_events = [e.to_dict() for e in events if e.is_new]
        all_new_events.extend(new_events)
    
    # æ”¶é›†ä»Šæ—¥äº‹ä»¶
    today_events = []
    for platform in platforms:
        events = load_platform_data(platform, current_path)
        platform_today_events = [e.to_dict() for e in events if e.event_date == today]
        today_events.extend(platform_today_events)
    
    # æŒ‰é‡è¦æ€§æ’åº
    all_new_events.sort(key=lambda x: x.get('importance', 0) or 0, reverse=True)
    today_events.sort(key=lambda x: x.get('importance', 0) or 0, reverse=True)
    
    # æ„å»ºå¯¼å‡ºæ•°æ®
    export_data = {
        'new_events': all_new_events[:100],  # æœ€å¤šå¯¼å‡º100ä¸ªæ–°å¢äº‹ä»¶
        'today_events': today_events,
        'export_time': datetime.now().isoformat(),
        'today': today
    }
    
    # ä¿å­˜åˆ°å¯¼å‡ºç›®å½•
    with open(os.path.join(export_dir, "latest_events.json"), 'w', encoding='utf-8') as f:
        f.write(json.dumps(export_data, ensure_ascii=False, indent=2))
    
    print(f"   ğŸ“… æœ€æ–°äº‹ä»¶å·²å¯¼å‡º (æ–°å¢: {len(all_new_events)}, ä»Šæ—¥: {len(today_events)})")

def export_calendar_data(export_dir, today):
    """å¯¼å‡ºæ—¥å†æ•°æ®"""
    current_path = "./data/active/current"
    platforms = ['cls', 'jiuyangongshe', 'tonghuashun', 'investing', 'eastmoney']
    
    # è®¡ç®—å¯¼å‡ºèŒƒå›´ï¼šä»Šå¤©å¼€å§‹çš„30å¤©
    start_date = today
    end_date = (datetime.strptime(today, '%Y-%m-%d') + timedelta(days=30)).strftime('%Y-%m-%d')
    
    # æŒ‰æ—¥æœŸç»„ç»‡äº‹ä»¶
    calendar_data = {}
    
    for platform in platforms:
        events = load_platform_data(platform, current_path)
        
        for event in events:
            if start_date <= event.event_date <= end_date:
                if event.event_date not in calendar_data:
                    calendar_data[event.event_date] = []
                
                # ç®€åŒ–äº‹ä»¶æ•°æ®ï¼Œå‡å°æ–‡ä»¶å¤§å°
                simplified_event = {
                    'event_id': event.event_id,
                    'platform': event.platform,
                    'title': event.title,
                    'event_time': event.event_time,
                    'importance': event.importance,
                    'category': event.category,
                    'country': event.country,
                    'is_new': event.is_new
                }
                
                calendar_data[event.event_date].append(simplified_event)
    
    # å¯¹æ¯å¤©çš„äº‹ä»¶æŒ‰é‡è¦æ€§æ’åº
    for date, events in calendar_data.items():
        events.sort(key=lambda x: x.get('importance', 0) or 0, reverse=True)
    
    # æ„å»ºå¯¼å‡ºæ•°æ®
    export_data = {
        'start_date': start_date,
        'end_date': end_date,
        'days': calendar_data,
        'export_time': datetime.now().isoformat()
    }
    
    # ä¿å­˜åˆ°å¯¼å‡ºç›®å½•
    with open(os.path.join(export_dir, "calendar_data.json"), 'w', encoding='utf-8') as f:
        f.write(json.dumps(export_data, ensure_ascii=False, indent=2))
    
    # ç»Ÿè®¡æ€»äº‹ä»¶æ•°
    total_events = sum(len(events) for events in calendar_data.values())
    print(f"   ğŸ—“ï¸ æ—¥å†æ•°æ®å·²å¯¼å‡º ({len(calendar_data)} å¤©, {total_events} ä¸ªäº‹ä»¶)")

# ============================================================================
# ç¨‹åºå…¥å£
# ============================================================================

if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='æŠ•èµ„æ—¥å†æ—¥å¸¸æ•°æ®é‡‡é›†ä¸å˜æ›´æ£€æµ‹')
    parser.add_argument('--first-run', action='store_true', help='é¦–æ¬¡è¿è¡Œæ¨¡å¼')
    parser.add_argument('--daily', action='store_true', help='æ—¥å¸¸æ›´æ–°æ¨¡å¼')
    parser.add_argument('--collect', action='store_true', help='åªé‡‡é›†æ•°æ®')
    parser.add_argument('--detect', action='store_true', help='åªæ£€æµ‹å˜æ›´')
    parser.add_argument('--export', action='store_true', help='å¯¼å‡ºæ•°æ®ä¾›ç½‘é¡µä½¿ç”¨')
    parser.add_argument('--status', action='store_true', help='æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import requests
        from bs4 import BeautifulSoup
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–åŒ…: {e}")
        print("è¯·è¿è¡Œ: pip install requests beautifulsoup4")
        sys.exit(1)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å†å²æ•°æ®
    archived_path = "./data/archived"
    if not os.path.exists(archived_path) or not any(
        f.endswith('.txt') for root, dirs, files in os.walk(archived_path) for f in files
    ):
        print("âŒ æœªå‘ç°å†å²æ•°æ®ï¼")
        print("è¯·å…ˆè¿è¡Œ: python historical_collector.py")
        sys.exit(1)
    
    print("ğŸ”„ æŠ•èµ„æ—¥å†æ—¥å¸¸æ•°æ®ç®¡ç†ç³»ç»Ÿ")
    print("=" * 50)
    
    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°æ‰§è¡Œç›¸åº”åŠŸèƒ½
    if args.first_run:
        # é¦–æ¬¡è¿è¡Œæ¨¡å¼
        scheduler = DailyTaskScheduler()
        success = scheduler.run_first_time()
        
        if success and args.export:
            export_data_for_web()
    
    elif args.daily:
        # æ‰§è¡Œå®Œæ•´æ—¥å¸¸æ›´æ–°
        scheduler = DailyTaskScheduler()
        success = scheduler.run_daily_update()
        
        if success and args.export:
            export_data_for_web()
    
    elif args.collect:
        # åªé‡‡é›†æœªæ¥æ•°æ®
        collector = FutureDataCollector()
        storage = DataStorage()
        future_data = collector.collect_all_future_data()
        storage.save_all_data(future_data)
        
        if args.export:
            export_data_for_web()
    
    elif args.detect:
        # åªæ‰§è¡Œå˜æ›´æ£€æµ‹
        collector = FutureDataCollector()
        detector = ChangeDetectionEngine()
        future_data = collector.collect_all_future_data()
        detector.detect_all_changes_with_new_data(future_data)
    
    elif args.export:
        # åªå¯¼å‡ºæ•°æ®
        export_data_for_web()
    
    elif args.status:
        # æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€
        print("\nğŸ“Š ç³»ç»ŸçŠ¶æ€:")
        print("-" * 40)
        
        # æ£€æŸ¥æ´»è·ƒæ•°æ®
        current_path = "./data/active/current"
        if os.path.exists(current_path):
            txt_files = [f for f in os.listdir(current_path) if f.endswith('.txt') and f not in ['metadata.txt', 'first_run_marker.txt']]
            print(f"æ´»è·ƒæ•°æ®å¹³å°: {len(txt_files)} ä¸ª")
            
            total_active_events = 0
            total_new_events = 0
            
            for txt_file in txt_files:
                platform = txt_file.replace('.txt', '')
                events = load_platform_data(platform, current_path)
                new_events = [e for e in events if e.is_new]
                
                platform_name = {
                    'cls': 'è´¢è”ç¤¾',
                    'jiuyangongshe': 'éŸ­ç ”å…¬ç¤¾',
                    'tonghuashun': 'åŒèŠ±é¡º',
                    'investing': 'è‹±ä¸ºè´¢æƒ…',
                    'eastmoney': 'ä¸œæ–¹è´¢å¯Œ'
                }.get(platform, platform)
                
                print(f"  {platform_name}: {len(events)} ä¸ªäº‹ä»¶ (æ–°å¢: {len(new_events)})")
                total_active_events += len(events)
                total_new_events += len(new_events)
            
            print(f"\næ´»è·ƒæ•°æ®æ€»è®¡: {total_active_events} ä¸ªäº‹ä»¶")
            if total_new_events > 0:
                print(f"æ–°å¢äº‹ä»¶æ€»è®¡: {total_new_events} ä¸ª")
        
        # æ£€æŸ¥å†å²æ•°æ®
        archived_path = "./data/archived"
        if os.path.exists(archived_path):
            total_archived_events = 0
            archived_months = 0
            archived_files = 0
            
            for year_dir in os.listdir(archived_path):
                if year_dir.isdigit():
                    year_path = os.path.join(archived_path, year_dir)
                    for month_dir in os.listdir(year_path):
                        if month_dir.endswith('æœˆ'):
                            archived_months += 1
                            month_path = os.path.join(year_path, month_dir)
                            txt_files = [f for f in os.listdir(month_path) if f.endswith('.txt')]
                            archived_files += len(txt_files)
                            
                            # ç»Ÿè®¡å†å²äº‹ä»¶æ•°é‡
                            for txt_file in txt_files:
                                platform = txt_file.replace('.txt', '')
                                try:
                                    events = load_platform_data(platform, month_path)
                                    total_archived_events += len(events)
                                except:
                                    pass
            
            print(f"\nå†å²æ•°æ®: {archived_months} ä¸ªæœˆ, {archived_files} ä¸ªæ–‡ä»¶")
            print(f"å†å²äº‹ä»¶æ€»è®¡: {total_archived_events} ä¸ª")
        
        # æ£€æŸ¥æ¯”è¾ƒåŸºå‡†
        previous_path = "./data/active/previous"
        if os.path.exists(previous_path):
            txt_files = [f for f in os.listdir(previous_path) if f.endswith('.txt') and f != 'metadata.txt']
            
            total_previous_events = 0
            for txt_file in txt_files:
                platform = txt_file.replace('.txt', '')
                try:
                    events = load_platform_data(platform, previous_path)
                    total_previous_events += len(events)
                except:
                    pass
            
            print(f"æ¯”è¾ƒåŸºå‡†: {len(txt_files)} ä¸ªå¹³å°, {total_previous_events} ä¸ªäº‹ä»¶")
        
        # æ£€æŸ¥æ•°æ®æ—¥æœŸèŒƒå›´
        print(f"\nğŸ“… æ•°æ®æ—¥æœŸèŒƒå›´:")
        try:
            all_events = []
            for platform in ['cls', 'jiuyangongshe', 'tonghuashun', 'investing', 'eastmoney']:
                events = load_platform_data(platform, current_path)
                all_events.extend(events)
            
            if all_events:
                dates = [event.event_date for event in all_events if event.event_date]
                if dates:
                    min_date = min(dates)
                    max_date = max(dates)
                    print(f"æ´»è·ƒæ•°æ®èŒƒå›´: {min_date} è‡³ {max_date}")
        except:
            pass
    
    else:
        # é»˜è®¤ï¼šè‡ªåŠ¨åˆ¤æ–­è¿è¡Œæ¨¡å¼
        scheduler = DailyTaskScheduler()
        if scheduler._is_first_run():
            print("ğŸ” æ£€æµ‹åˆ°é¦–æ¬¡è¿è¡Œï¼Œæ‰§è¡Œé¦–æ¬¡é‡‡é›†...")
            success = scheduler.run_first_time()
        else:
            print("ğŸ”„ æ‰§è¡Œæ—¥å¸¸æ›´æ–°...")
            success = scheduler.run_daily_update()
        
        if success:
            export_data_for_web()

        
